{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-16T04:32:22.776403700Z",
     "start_time": "2023-05-16T04:32:22.392292300Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('project_data_files/book_rating_train.csv')\n",
    "test_df = pd.read_csv('project_data_files/book_rating_test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:29.111864Z",
     "start_time": "2023-05-17T06:28:28.793548900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2160x720 with 6 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distribution of each feature\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(30, 10))\n",
    "sns.countplot(x=\"Language\", data=train_df, ax=axs[0, 0])\n",
    "sns.countplot(x=\"PublishDay\", data=train_df, ax=axs[0, 1])\n",
    "sns.countplot(x=\"PublishMonth\", data=train_df, ax=axs[0, 2])\n",
    "sns.countplot(x=\"PublishYear\", data=train_df, ax=axs[1, 0])\n",
    "sns.countplot(x=\"pagesNumber\", data=train_df, ax=axs[1, 1])\n",
    "sns.countplot(x=\"rating_label\", data=train_df, ax=axs[1, 2])\n",
    "\n",
    "# tilt x-axis labels\n",
    "for ax in axs.flat:\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T04:48:11.609562700Z",
     "start_time": "2023-05-16T04:47:56.618161200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "            Publisher  pagesNumber  rating_label\n0  Teaching Resources           48           4.0\n1           DoubleDay          364           4.0\n2     Chronicle Books           32           4.0\n3         Bison Books          293           4.0\n4   Penguin Books Ltd          352           3.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>rating_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Teaching Resources</td>\n      <td>48</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DoubleDay</td>\n      <td>364</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(columns=['Language', 'PublishDay', 'Name', 'Authors', 'Description', 'PublishYear', 'PublishMonth'], axis=1)\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:31.639012600Z",
     "start_time": "2023-05-17T06:28:31.543575600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "            Publisher  pagesNumber  rating_label  Author_0  Author_1  \\\n0  Teaching Resources           48           4.0  0.359375 -0.096944   \n1           DoubleDay          364           4.0 -0.074845  0.060063   \n2     Chronicle Books           32           4.0 -0.127589 -0.100911   \n3         Bison Books          293           4.0 -0.000472 -0.048197   \n4   Penguin Books Ltd          352           3.0 -0.162106 -0.023212   \n\n   Author_2  Author_3  Author_4  Author_5  Author_6  ...  Author_10  \\\n0  0.021326  0.304888 -0.084434 -0.138658  0.306277  ...   0.329671   \n1  0.132891  0.051957  0.127083  0.017997  0.172967  ...   0.400349   \n2  0.158580  0.046532 -0.065661 -0.037972 -0.051163  ...   0.225617   \n3  0.106046 -0.100795 -0.147681 -0.017288 -0.133912  ...   0.133304   \n4  0.189444 -0.042658 -0.117135 -0.075968 -0.005331  ...   0.224210   \n\n   Author_11  Author_12  Author_13  Author_14  Author_15  Author_16  \\\n0   0.343979   0.018261   0.115687  -0.111172   0.068306   0.158065   \n1   0.065201   0.349188   0.020555   0.281087   0.231422   0.129853   \n2  -0.004355   0.173353   0.087015   0.106534   0.040950   0.209152   \n3  -0.069995   0.206028   0.089625   0.157605   0.131767   0.244849   \n4   0.049880   0.003623   0.062291  -0.030742   0.130882   0.295086   \n\n   Author_17  Author_18  Author_19  \n0   0.053510  -0.136804  -0.084448  \n1  -0.213233  -0.081253  -0.204687  \n2  -0.215313  -0.177547  -0.178094  \n3  -0.321698  -0.198365  -0.208098  \n4  -0.061550  -0.244197  -0.272161  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>rating_label</th>\n      <th>Author_0</th>\n      <th>Author_1</th>\n      <th>Author_2</th>\n      <th>Author_3</th>\n      <th>Author_4</th>\n      <th>Author_5</th>\n      <th>Author_6</th>\n      <th>...</th>\n      <th>Author_10</th>\n      <th>Author_11</th>\n      <th>Author_12</th>\n      <th>Author_13</th>\n      <th>Author_14</th>\n      <th>Author_15</th>\n      <th>Author_16</th>\n      <th>Author_17</th>\n      <th>Author_18</th>\n      <th>Author_19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Teaching Resources</td>\n      <td>48</td>\n      <td>4.0</td>\n      <td>0.359375</td>\n      <td>-0.096944</td>\n      <td>0.021326</td>\n      <td>0.304888</td>\n      <td>-0.084434</td>\n      <td>-0.138658</td>\n      <td>0.306277</td>\n      <td>...</td>\n      <td>0.329671</td>\n      <td>0.343979</td>\n      <td>0.018261</td>\n      <td>0.115687</td>\n      <td>-0.111172</td>\n      <td>0.068306</td>\n      <td>0.158065</td>\n      <td>0.053510</td>\n      <td>-0.136804</td>\n      <td>-0.084448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DoubleDay</td>\n      <td>364</td>\n      <td>4.0</td>\n      <td>-0.074845</td>\n      <td>0.060063</td>\n      <td>0.132891</td>\n      <td>0.051957</td>\n      <td>0.127083</td>\n      <td>0.017997</td>\n      <td>0.172967</td>\n      <td>...</td>\n      <td>0.400349</td>\n      <td>0.065201</td>\n      <td>0.349188</td>\n      <td>0.020555</td>\n      <td>0.281087</td>\n      <td>0.231422</td>\n      <td>0.129853</td>\n      <td>-0.213233</td>\n      <td>-0.081253</td>\n      <td>-0.204687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32</td>\n      <td>4.0</td>\n      <td>-0.127589</td>\n      <td>-0.100911</td>\n      <td>0.158580</td>\n      <td>0.046532</td>\n      <td>-0.065661</td>\n      <td>-0.037972</td>\n      <td>-0.051163</td>\n      <td>...</td>\n      <td>0.225617</td>\n      <td>-0.004355</td>\n      <td>0.173353</td>\n      <td>0.087015</td>\n      <td>0.106534</td>\n      <td>0.040950</td>\n      <td>0.209152</td>\n      <td>-0.215313</td>\n      <td>-0.177547</td>\n      <td>-0.178094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293</td>\n      <td>4.0</td>\n      <td>-0.000472</td>\n      <td>-0.048197</td>\n      <td>0.106046</td>\n      <td>-0.100795</td>\n      <td>-0.147681</td>\n      <td>-0.017288</td>\n      <td>-0.133912</td>\n      <td>...</td>\n      <td>0.133304</td>\n      <td>-0.069995</td>\n      <td>0.206028</td>\n      <td>0.089625</td>\n      <td>0.157605</td>\n      <td>0.131767</td>\n      <td>0.244849</td>\n      <td>-0.321698</td>\n      <td>-0.198365</td>\n      <td>-0.208098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352</td>\n      <td>3.0</td>\n      <td>-0.162106</td>\n      <td>-0.023212</td>\n      <td>0.189444</td>\n      <td>-0.042658</td>\n      <td>-0.117135</td>\n      <td>-0.075968</td>\n      <td>-0.005331</td>\n      <td>...</td>\n      <td>0.224210</td>\n      <td>0.049880</td>\n      <td>0.003623</td>\n      <td>0.062291</td>\n      <td>-0.030742</td>\n      <td>0.130882</td>\n      <td>0.295086</td>\n      <td>-0.061550</td>\n      <td>-0.244197</td>\n      <td>-0.272161</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  23 columns</p>\n</div>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "for i in range(20):\n",
    "    train_df['Author_' + str(i)] = author_vec[i]\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:33.199526700Z",
     "start_time": "2023-05-17T06:28:33.046310600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\612441367.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['Name_' + str(i)] = name_vec[i]\n"
     ]
    },
    {
     "data": {
      "text/plain": "            Publisher  pagesNumber  rating_label  Author_0  Author_1  \\\n0  Teaching Resources           48           4.0  0.359375 -0.096944   \n1           DoubleDay          364           4.0 -0.074845  0.060063   \n2     Chronicle Books           32           4.0 -0.127589 -0.100911   \n3         Bison Books          293           4.0 -0.000472 -0.048197   \n4   Penguin Books Ltd          352           3.0 -0.162106 -0.023212   \n\n   Author_2  Author_3  Author_4  Author_5  Author_6  ...   Name_90   Name_91  \\\n0  0.021326  0.304888 -0.084434 -0.138658  0.306277  ... -0.172811  0.098389   \n1  0.132891  0.051957  0.127083  0.017997  0.172967  ...  0.245650 -0.049657   \n2  0.158580  0.046532 -0.065661 -0.037972 -0.051163  ... -0.033781  0.093943   \n3  0.106046 -0.100795 -0.147681 -0.017288 -0.133912  ...  0.020762 -0.149720   \n4  0.189444 -0.042658 -0.117135 -0.075968 -0.005331  ...  0.191644  0.044182   \n\n    Name_92   Name_93   Name_94   Name_95   Name_96   Name_97   Name_98  \\\n0 -0.062941  0.118057 -0.065377  0.227973  0.218879 -0.151266 -0.048105   \n1  0.072740 -0.055925 -0.000046  0.140500  0.067133 -0.238091  0.109774   \n2  0.132654  0.030295  0.102714  0.154334  0.129325 -0.231493  0.007541   \n3  0.150557  0.294355  0.001157  0.285179  0.049340 -0.037548  0.042920   \n4  0.054631 -0.025782  0.049917  0.122052 -0.084216 -0.096424 -0.068681   \n\n    Name_99  \n0  0.300822  \n1 -0.156772  \n2 -0.098540  \n3  0.176173  \n4 -0.005293  \n\n[5 rows x 123 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>rating_label</th>\n      <th>Author_0</th>\n      <th>Author_1</th>\n      <th>Author_2</th>\n      <th>Author_3</th>\n      <th>Author_4</th>\n      <th>Author_5</th>\n      <th>Author_6</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Teaching Resources</td>\n      <td>48</td>\n      <td>4.0</td>\n      <td>0.359375</td>\n      <td>-0.096944</td>\n      <td>0.021326</td>\n      <td>0.304888</td>\n      <td>-0.084434</td>\n      <td>-0.138658</td>\n      <td>0.306277</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DoubleDay</td>\n      <td>364</td>\n      <td>4.0</td>\n      <td>-0.074845</td>\n      <td>0.060063</td>\n      <td>0.132891</td>\n      <td>0.051957</td>\n      <td>0.127083</td>\n      <td>0.017997</td>\n      <td>0.172967</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32</td>\n      <td>4.0</td>\n      <td>-0.127589</td>\n      <td>-0.100911</td>\n      <td>0.158580</td>\n      <td>0.046532</td>\n      <td>-0.065661</td>\n      <td>-0.037972</td>\n      <td>-0.051163</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293</td>\n      <td>4.0</td>\n      <td>-0.000472</td>\n      <td>-0.048197</td>\n      <td>0.106046</td>\n      <td>-0.100795</td>\n      <td>-0.147681</td>\n      <td>-0.017288</td>\n      <td>-0.133912</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352</td>\n      <td>3.0</td>\n      <td>-0.162106</td>\n      <td>-0.023212</td>\n      <td>0.189444</td>\n      <td>-0.042658</td>\n      <td>-0.117135</td>\n      <td>-0.075968</td>\n      <td>-0.005331</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  123 columns</p>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "for i in range(100):\n",
    "    train_df['Name_' + str(i)] = name_vec[i]\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:34.962542500Z",
     "start_time": "2023-05-17T06:28:33.939581300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "train_df['pagesNumber'] = train_df['pagesNumber'].apply(lambda x: np.nan if x < 10 else x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:36.243301Z",
     "start_time": "2023-05-17T06:28:36.167612800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "                             Publisher  pagesNumber  rating_label  Author_0  \\\n0                   Teaching Resources         48.0           4.0  0.359375   \n1                            DoubleDay        364.0           4.0 -0.074845   \n2                      Chronicle Books         32.0           4.0 -0.127589   \n3                          Bison Books        293.0           4.0 -0.000472   \n4                    Penguin Books Ltd        352.0           3.0 -0.162106   \n...                                ...          ...           ...       ...   \n23058                          2.13.61        120.0           4.0 -0.194677   \n23059      HMH Books for Young Readers         32.0           4.0 -0.115993   \n23060  Rowman & Littlefield Publishers        132.0           4.0 -0.126878   \n23061              New Amsterdam Books        136.0           4.0 -0.134530   \n23062                           Puffin        192.0           4.0 -0.204117   \n\n       Author_1  Author_2  Author_3  Author_4  Author_5  Author_6  ...  \\\n0     -0.096944  0.021326  0.304888 -0.084434 -0.138658  0.306277  ...   \n1      0.060063  0.132891  0.051957  0.127083  0.017997  0.172967  ...   \n2     -0.100911  0.158580  0.046532 -0.065661 -0.037972 -0.051163  ...   \n3     -0.048197  0.106046 -0.100795 -0.147681 -0.017288 -0.133912  ...   \n4     -0.023212  0.189444 -0.042658 -0.117135 -0.075968 -0.005331  ...   \n...         ...       ...       ...       ...       ...       ...  ...   \n23058  0.063026  0.125115 -0.041354 -0.122502 -0.207333 -0.042279  ...   \n23059 -0.003955 -0.027285 -0.032830  0.091905 -0.257285 -0.045624  ...   \n23060 -0.120418  0.198828  0.093403 -0.053232 -0.114909 -0.013179  ...   \n23061 -0.061256  0.178935  0.057537 -0.045066 -0.088796 -0.038559  ...   \n23062 -0.007189  0.375681  0.011292 -0.341423 -0.061017  0.046779  ...   \n\n        Name_90   Name_91   Name_92   Name_93   Name_94   Name_95   Name_96  \\\n0     -0.172811  0.098389 -0.062941  0.118057 -0.065377  0.227973  0.218879   \n1      0.245650 -0.049657  0.072740 -0.055925 -0.000046  0.140500  0.067133   \n2     -0.033781  0.093943  0.132654  0.030295  0.102714  0.154334  0.129325   \n3      0.020762 -0.149720  0.150557  0.294355  0.001157  0.285179  0.049340   \n4      0.191644  0.044182  0.054631 -0.025782  0.049917  0.122052 -0.084216   \n...         ...       ...       ...       ...       ...       ...       ...   \n23058 -0.000418 -0.062899  0.048064  0.029612  0.191065  0.096081 -0.100516   \n23059  0.150964 -0.029046  0.171029 -0.072123 -0.004459  0.247430  0.111973   \n23060  0.193755 -0.118570  0.006740 -0.108623 -0.036143  0.168113  0.136478   \n23061  0.009007  0.154127  0.219128 -0.305824 -0.017904 -0.059886  0.108616   \n23062  0.068714 -0.065924  0.082228 -0.003849  0.099006  0.081608  0.094459   \n\n        Name_97   Name_98   Name_99  \n0     -0.151266 -0.048105  0.300822  \n1     -0.238091  0.109774 -0.156772  \n2     -0.231493  0.007541 -0.098540  \n3     -0.037548  0.042920  0.176173  \n4     -0.096424 -0.068681 -0.005293  \n...         ...       ...       ...  \n23058 -0.190299  0.224559  0.086601  \n23059  0.019573  0.070569 -0.112066  \n23060  0.087885  0.113180  0.000569  \n23061  0.041879 -0.138893 -0.044187  \n23062 -0.048776  0.032433  0.132977  \n\n[22626 rows x 123 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>rating_label</th>\n      <th>Author_0</th>\n      <th>Author_1</th>\n      <th>Author_2</th>\n      <th>Author_3</th>\n      <th>Author_4</th>\n      <th>Author_5</th>\n      <th>Author_6</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Teaching Resources</td>\n      <td>48.0</td>\n      <td>4.0</td>\n      <td>0.359375</td>\n      <td>-0.096944</td>\n      <td>0.021326</td>\n      <td>0.304888</td>\n      <td>-0.084434</td>\n      <td>-0.138658</td>\n      <td>0.306277</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DoubleDay</td>\n      <td>364.0</td>\n      <td>4.0</td>\n      <td>-0.074845</td>\n      <td>0.060063</td>\n      <td>0.132891</td>\n      <td>0.051957</td>\n      <td>0.127083</td>\n      <td>0.017997</td>\n      <td>0.172967</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>-0.127589</td>\n      <td>-0.100911</td>\n      <td>0.158580</td>\n      <td>0.046532</td>\n      <td>-0.065661</td>\n      <td>-0.037972</td>\n      <td>-0.051163</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293.0</td>\n      <td>4.0</td>\n      <td>-0.000472</td>\n      <td>-0.048197</td>\n      <td>0.106046</td>\n      <td>-0.100795</td>\n      <td>-0.147681</td>\n      <td>-0.017288</td>\n      <td>-0.133912</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352.0</td>\n      <td>3.0</td>\n      <td>-0.162106</td>\n      <td>-0.023212</td>\n      <td>0.189444</td>\n      <td>-0.042658</td>\n      <td>-0.117135</td>\n      <td>-0.075968</td>\n      <td>-0.005331</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23058</th>\n      <td>2.13.61</td>\n      <td>120.0</td>\n      <td>4.0</td>\n      <td>-0.194677</td>\n      <td>0.063026</td>\n      <td>0.125115</td>\n      <td>-0.041354</td>\n      <td>-0.122502</td>\n      <td>-0.207333</td>\n      <td>-0.042279</td>\n      <td>...</td>\n      <td>-0.000418</td>\n      <td>-0.062899</td>\n      <td>0.048064</td>\n      <td>0.029612</td>\n      <td>0.191065</td>\n      <td>0.096081</td>\n      <td>-0.100516</td>\n      <td>-0.190299</td>\n      <td>0.224559</td>\n      <td>0.086601</td>\n    </tr>\n    <tr>\n      <th>23059</th>\n      <td>HMH Books for Young Readers</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>-0.115993</td>\n      <td>-0.003955</td>\n      <td>-0.027285</td>\n      <td>-0.032830</td>\n      <td>0.091905</td>\n      <td>-0.257285</td>\n      <td>-0.045624</td>\n      <td>...</td>\n      <td>0.150964</td>\n      <td>-0.029046</td>\n      <td>0.171029</td>\n      <td>-0.072123</td>\n      <td>-0.004459</td>\n      <td>0.247430</td>\n      <td>0.111973</td>\n      <td>0.019573</td>\n      <td>0.070569</td>\n      <td>-0.112066</td>\n    </tr>\n    <tr>\n      <th>23060</th>\n      <td>Rowman &amp; Littlefield Publishers</td>\n      <td>132.0</td>\n      <td>4.0</td>\n      <td>-0.126878</td>\n      <td>-0.120418</td>\n      <td>0.198828</td>\n      <td>0.093403</td>\n      <td>-0.053232</td>\n      <td>-0.114909</td>\n      <td>-0.013179</td>\n      <td>...</td>\n      <td>0.193755</td>\n      <td>-0.118570</td>\n      <td>0.006740</td>\n      <td>-0.108623</td>\n      <td>-0.036143</td>\n      <td>0.168113</td>\n      <td>0.136478</td>\n      <td>0.087885</td>\n      <td>0.113180</td>\n      <td>0.000569</td>\n    </tr>\n    <tr>\n      <th>23061</th>\n      <td>New Amsterdam Books</td>\n      <td>136.0</td>\n      <td>4.0</td>\n      <td>-0.134530</td>\n      <td>-0.061256</td>\n      <td>0.178935</td>\n      <td>0.057537</td>\n      <td>-0.045066</td>\n      <td>-0.088796</td>\n      <td>-0.038559</td>\n      <td>...</td>\n      <td>0.009007</td>\n      <td>0.154127</td>\n      <td>0.219128</td>\n      <td>-0.305824</td>\n      <td>-0.017904</td>\n      <td>-0.059886</td>\n      <td>0.108616</td>\n      <td>0.041879</td>\n      <td>-0.138893</td>\n      <td>-0.044187</td>\n    </tr>\n    <tr>\n      <th>23062</th>\n      <td>Puffin</td>\n      <td>192.0</td>\n      <td>4.0</td>\n      <td>-0.204117</td>\n      <td>-0.007189</td>\n      <td>0.375681</td>\n      <td>0.011292</td>\n      <td>-0.341423</td>\n      <td>-0.061017</td>\n      <td>0.046779</td>\n      <td>...</td>\n      <td>0.068714</td>\n      <td>-0.065924</td>\n      <td>0.082228</td>\n      <td>-0.003849</td>\n      <td>0.099006</td>\n      <td>0.081608</td>\n      <td>0.094459</td>\n      <td>-0.048776</td>\n      <td>0.032433</td>\n      <td>0.132977</td>\n    </tr>\n  </tbody>\n</table>\n<p>22626 rows  123 columns</p>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = train_df['pagesNumber'].mean()\n",
    "sd = train_df['pagesNumber'].std()\n",
    "train_df = train_df[train_df['pagesNumber'] <= mean + 3 * sd]\n",
    "train_df = train_df[train_df['pagesNumber'] >= mean - 3 * sd]\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:37.917504200Z",
     "start_time": "2023-05-17T06:28:37.763685700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275.9122690709803\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             Publisher  pagesNumber  rating_label  Author_0  \\\n0                   Teaching Resources         48.0           4.0  0.359375   \n1                            DoubleDay        364.0           4.0 -0.074845   \n2                      Chronicle Books         32.0           4.0 -0.127589   \n3                          Bison Books        293.0           4.0 -0.000472   \n4                    Penguin Books Ltd        352.0           3.0 -0.162106   \n...                                ...          ...           ...       ...   \n23058                          2.13.61        120.0           4.0 -0.194677   \n23059      HMH Books for Young Readers         32.0           4.0 -0.115993   \n23060  Rowman & Littlefield Publishers        132.0           4.0 -0.126878   \n23061              New Amsterdam Books        136.0           4.0 -0.134530   \n23062                           Puffin        192.0           4.0 -0.204117   \n\n       Author_1  Author_2  Author_3  Author_4  Author_5  Author_6  ...  \\\n0     -0.096944  0.021326  0.304888 -0.084434 -0.138658  0.306277  ...   \n1      0.060063  0.132891  0.051957  0.127083  0.017997  0.172967  ...   \n2     -0.100911  0.158580  0.046532 -0.065661 -0.037972 -0.051163  ...   \n3     -0.048197  0.106046 -0.100795 -0.147681 -0.017288 -0.133912  ...   \n4     -0.023212  0.189444 -0.042658 -0.117135 -0.075968 -0.005331  ...   \n...         ...       ...       ...       ...       ...       ...  ...   \n23058  0.063026  0.125115 -0.041354 -0.122502 -0.207333 -0.042279  ...   \n23059 -0.003955 -0.027285 -0.032830  0.091905 -0.257285 -0.045624  ...   \n23060 -0.120418  0.198828  0.093403 -0.053232 -0.114909 -0.013179  ...   \n23061 -0.061256  0.178935  0.057537 -0.045066 -0.088796 -0.038559  ...   \n23062 -0.007189  0.375681  0.011292 -0.341423 -0.061017  0.046779  ...   \n\n        Name_90   Name_91   Name_92   Name_93   Name_94   Name_95   Name_96  \\\n0     -0.172811  0.098389 -0.062941  0.118057 -0.065377  0.227973  0.218879   \n1      0.245650 -0.049657  0.072740 -0.055925 -0.000046  0.140500  0.067133   \n2     -0.033781  0.093943  0.132654  0.030295  0.102714  0.154334  0.129325   \n3      0.020762 -0.149720  0.150557  0.294355  0.001157  0.285179  0.049340   \n4      0.191644  0.044182  0.054631 -0.025782  0.049917  0.122052 -0.084216   \n...         ...       ...       ...       ...       ...       ...       ...   \n23058 -0.000418 -0.062899  0.048064  0.029612  0.191065  0.096081 -0.100516   \n23059  0.150964 -0.029046  0.171029 -0.072123 -0.004459  0.247430  0.111973   \n23060  0.193755 -0.118570  0.006740 -0.108623 -0.036143  0.168113  0.136478   \n23061  0.009007  0.154127  0.219128 -0.305824 -0.017904 -0.059886  0.108616   \n23062  0.068714 -0.065924  0.082228 -0.003849  0.099006  0.081608  0.094459   \n\n        Name_97   Name_98   Name_99  \n0     -0.151266 -0.048105  0.300822  \n1     -0.238091  0.109774 -0.156772  \n2     -0.231493  0.007541 -0.098540  \n3     -0.037548  0.042920  0.176173  \n4     -0.096424 -0.068681 -0.005293  \n...         ...       ...       ...  \n23058 -0.190299  0.224559  0.086601  \n23059  0.019573  0.070569 -0.112066  \n23060  0.087885  0.113180  0.000569  \n23061  0.041879 -0.138893 -0.044187  \n23062 -0.048776  0.032433  0.132977  \n\n[22626 rows x 123 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>rating_label</th>\n      <th>Author_0</th>\n      <th>Author_1</th>\n      <th>Author_2</th>\n      <th>Author_3</th>\n      <th>Author_4</th>\n      <th>Author_5</th>\n      <th>Author_6</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Teaching Resources</td>\n      <td>48.0</td>\n      <td>4.0</td>\n      <td>0.359375</td>\n      <td>-0.096944</td>\n      <td>0.021326</td>\n      <td>0.304888</td>\n      <td>-0.084434</td>\n      <td>-0.138658</td>\n      <td>0.306277</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DoubleDay</td>\n      <td>364.0</td>\n      <td>4.0</td>\n      <td>-0.074845</td>\n      <td>0.060063</td>\n      <td>0.132891</td>\n      <td>0.051957</td>\n      <td>0.127083</td>\n      <td>0.017997</td>\n      <td>0.172967</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>-0.127589</td>\n      <td>-0.100911</td>\n      <td>0.158580</td>\n      <td>0.046532</td>\n      <td>-0.065661</td>\n      <td>-0.037972</td>\n      <td>-0.051163</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293.0</td>\n      <td>4.0</td>\n      <td>-0.000472</td>\n      <td>-0.048197</td>\n      <td>0.106046</td>\n      <td>-0.100795</td>\n      <td>-0.147681</td>\n      <td>-0.017288</td>\n      <td>-0.133912</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352.0</td>\n      <td>3.0</td>\n      <td>-0.162106</td>\n      <td>-0.023212</td>\n      <td>0.189444</td>\n      <td>-0.042658</td>\n      <td>-0.117135</td>\n      <td>-0.075968</td>\n      <td>-0.005331</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23058</th>\n      <td>2.13.61</td>\n      <td>120.0</td>\n      <td>4.0</td>\n      <td>-0.194677</td>\n      <td>0.063026</td>\n      <td>0.125115</td>\n      <td>-0.041354</td>\n      <td>-0.122502</td>\n      <td>-0.207333</td>\n      <td>-0.042279</td>\n      <td>...</td>\n      <td>-0.000418</td>\n      <td>-0.062899</td>\n      <td>0.048064</td>\n      <td>0.029612</td>\n      <td>0.191065</td>\n      <td>0.096081</td>\n      <td>-0.100516</td>\n      <td>-0.190299</td>\n      <td>0.224559</td>\n      <td>0.086601</td>\n    </tr>\n    <tr>\n      <th>23059</th>\n      <td>HMH Books for Young Readers</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>-0.115993</td>\n      <td>-0.003955</td>\n      <td>-0.027285</td>\n      <td>-0.032830</td>\n      <td>0.091905</td>\n      <td>-0.257285</td>\n      <td>-0.045624</td>\n      <td>...</td>\n      <td>0.150964</td>\n      <td>-0.029046</td>\n      <td>0.171029</td>\n      <td>-0.072123</td>\n      <td>-0.004459</td>\n      <td>0.247430</td>\n      <td>0.111973</td>\n      <td>0.019573</td>\n      <td>0.070569</td>\n      <td>-0.112066</td>\n    </tr>\n    <tr>\n      <th>23060</th>\n      <td>Rowman &amp; Littlefield Publishers</td>\n      <td>132.0</td>\n      <td>4.0</td>\n      <td>-0.126878</td>\n      <td>-0.120418</td>\n      <td>0.198828</td>\n      <td>0.093403</td>\n      <td>-0.053232</td>\n      <td>-0.114909</td>\n      <td>-0.013179</td>\n      <td>...</td>\n      <td>0.193755</td>\n      <td>-0.118570</td>\n      <td>0.006740</td>\n      <td>-0.108623</td>\n      <td>-0.036143</td>\n      <td>0.168113</td>\n      <td>0.136478</td>\n      <td>0.087885</td>\n      <td>0.113180</td>\n      <td>0.000569</td>\n    </tr>\n    <tr>\n      <th>23061</th>\n      <td>New Amsterdam Books</td>\n      <td>136.0</td>\n      <td>4.0</td>\n      <td>-0.134530</td>\n      <td>-0.061256</td>\n      <td>0.178935</td>\n      <td>0.057537</td>\n      <td>-0.045066</td>\n      <td>-0.088796</td>\n      <td>-0.038559</td>\n      <td>...</td>\n      <td>0.009007</td>\n      <td>0.154127</td>\n      <td>0.219128</td>\n      <td>-0.305824</td>\n      <td>-0.017904</td>\n      <td>-0.059886</td>\n      <td>0.108616</td>\n      <td>0.041879</td>\n      <td>-0.138893</td>\n      <td>-0.044187</td>\n    </tr>\n    <tr>\n      <th>23062</th>\n      <td>Puffin</td>\n      <td>192.0</td>\n      <td>4.0</td>\n      <td>-0.204117</td>\n      <td>-0.007189</td>\n      <td>0.375681</td>\n      <td>0.011292</td>\n      <td>-0.341423</td>\n      <td>-0.061017</td>\n      <td>0.046779</td>\n      <td>...</td>\n      <td>0.068714</td>\n      <td>-0.065924</td>\n      <td>0.082228</td>\n      <td>-0.003849</td>\n      <td>0.099006</td>\n      <td>0.081608</td>\n      <td>0.094459</td>\n      <td>-0.048776</td>\n      <td>0.032433</td>\n      <td>0.132977</td>\n    </tr>\n  </tbody>\n</table>\n<p>22626 rows  123 columns</p>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = train_df['pagesNumber'].mean()\n",
    "print(mean)\n",
    "train_df['pagesNumber'] = train_df['pagesNumber'].fillna(mean)\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:39.534727400Z",
     "start_time": "2023-05-17T06:28:39.450480600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "train_df['Publisher'] = train_df['Publisher'].fillna('Unknown')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:41.005825800Z",
     "start_time": "2023-05-17T06:28:40.922589900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "data_df = train_df['Publisher'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:41.434075600Z",
     "start_time": "2023-05-17T06:28:41.358048200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# mark publishers with less than 10 books as 'Other'\n",
    "train_df['Publisher'] = train_df['Publisher'].apply(lambda x: 'Other' if x in data_df[data_df < 10].index else x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:47.846563600Z",
     "start_time": "2023-05-17T06:28:42.604696700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "y_train = train_df['rating_label']\n",
    "# y_test = test_df['rating_label']\n",
    "\n",
    "X_train = train_df.drop(['rating_label'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:48.634711100Z",
     "start_time": "2023-05-17T06:28:48.544750700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "                             Publisher  pagesNumber  Author_0  Author_1  \\\n0                                Other         48.0  0.359375 -0.096944   \n1                                Other        364.0 -0.074845  0.060063   \n2                      Chronicle Books         32.0 -0.127589 -0.100911   \n3                          Bison Books        293.0 -0.000472 -0.048197   \n4                    Penguin Books Ltd        352.0 -0.162106 -0.023212   \n...                                ...          ...       ...       ...   \n23058                            Other        120.0 -0.194677  0.063026   \n23059      HMH Books for Young Readers         32.0 -0.115993 -0.003955   \n23060  Rowman & Littlefield Publishers        132.0 -0.126878 -0.120418   \n23061                            Other        136.0 -0.134530 -0.061256   \n23062                           Puffin        192.0 -0.204117 -0.007189   \n\n       Author_2  Author_3  Author_4  Author_5  Author_6  Author_7  ...  \\\n0      0.021326  0.304888 -0.084434 -0.138658  0.306277  0.418431  ...   \n1      0.132891  0.051957  0.127083  0.017997  0.172967  0.376880  ...   \n2      0.158580  0.046532 -0.065661 -0.037972 -0.051163  0.217950  ...   \n3      0.106046 -0.100795 -0.147681 -0.017288 -0.133912  0.229301  ...   \n4      0.189444 -0.042658 -0.117135 -0.075968 -0.005331  0.141808  ...   \n...         ...       ...       ...       ...       ...       ...  ...   \n23058  0.125115 -0.041354 -0.122502 -0.207333 -0.042279  0.283049  ...   \n23059 -0.027285 -0.032830  0.091905 -0.257285 -0.045624  0.374351  ...   \n23060  0.198828  0.093403 -0.053232 -0.114909 -0.013179  0.289315  ...   \n23061  0.178935  0.057537 -0.045066 -0.088796 -0.038559  0.219906  ...   \n23062  0.375681  0.011292 -0.341423 -0.061017  0.046779  0.103854  ...   \n\n        Name_90   Name_91   Name_92   Name_93   Name_94   Name_95   Name_96  \\\n0     -0.172811  0.098389 -0.062941  0.118057 -0.065377  0.227973  0.218879   \n1      0.245650 -0.049657  0.072740 -0.055925 -0.000046  0.140500  0.067133   \n2     -0.033781  0.093943  0.132654  0.030295  0.102714  0.154334  0.129325   \n3      0.020762 -0.149720  0.150557  0.294355  0.001157  0.285179  0.049340   \n4      0.191644  0.044182  0.054631 -0.025782  0.049917  0.122052 -0.084216   \n...         ...       ...       ...       ...       ...       ...       ...   \n23058 -0.000418 -0.062899  0.048064  0.029612  0.191065  0.096081 -0.100516   \n23059  0.150964 -0.029046  0.171029 -0.072123 -0.004459  0.247430  0.111973   \n23060  0.193755 -0.118570  0.006740 -0.108623 -0.036143  0.168113  0.136478   \n23061  0.009007  0.154127  0.219128 -0.305824 -0.017904 -0.059886  0.108616   \n23062  0.068714 -0.065924  0.082228 -0.003849  0.099006  0.081608  0.094459   \n\n        Name_97   Name_98   Name_99  \n0     -0.151266 -0.048105  0.300822  \n1     -0.238091  0.109774 -0.156772  \n2     -0.231493  0.007541 -0.098540  \n3     -0.037548  0.042920  0.176173  \n4     -0.096424 -0.068681 -0.005293  \n...         ...       ...       ...  \n23058 -0.190299  0.224559  0.086601  \n23059  0.019573  0.070569 -0.112066  \n23060  0.087885  0.113180  0.000569  \n23061  0.041879 -0.138893 -0.044187  \n23062 -0.048776  0.032433  0.132977  \n\n[22626 rows x 122 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n      <th>Author_0</th>\n      <th>Author_1</th>\n      <th>Author_2</th>\n      <th>Author_3</th>\n      <th>Author_4</th>\n      <th>Author_5</th>\n      <th>Author_6</th>\n      <th>Author_7</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Other</td>\n      <td>48.0</td>\n      <td>0.359375</td>\n      <td>-0.096944</td>\n      <td>0.021326</td>\n      <td>0.304888</td>\n      <td>-0.084434</td>\n      <td>-0.138658</td>\n      <td>0.306277</td>\n      <td>0.418431</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Other</td>\n      <td>364.0</td>\n      <td>-0.074845</td>\n      <td>0.060063</td>\n      <td>0.132891</td>\n      <td>0.051957</td>\n      <td>0.127083</td>\n      <td>0.017997</td>\n      <td>0.172967</td>\n      <td>0.376880</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chronicle Books</td>\n      <td>32.0</td>\n      <td>-0.127589</td>\n      <td>-0.100911</td>\n      <td>0.158580</td>\n      <td>0.046532</td>\n      <td>-0.065661</td>\n      <td>-0.037972</td>\n      <td>-0.051163</td>\n      <td>0.217950</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bison Books</td>\n      <td>293.0</td>\n      <td>-0.000472</td>\n      <td>-0.048197</td>\n      <td>0.106046</td>\n      <td>-0.100795</td>\n      <td>-0.147681</td>\n      <td>-0.017288</td>\n      <td>-0.133912</td>\n      <td>0.229301</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Penguin Books Ltd</td>\n      <td>352.0</td>\n      <td>-0.162106</td>\n      <td>-0.023212</td>\n      <td>0.189444</td>\n      <td>-0.042658</td>\n      <td>-0.117135</td>\n      <td>-0.075968</td>\n      <td>-0.005331</td>\n      <td>0.141808</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23058</th>\n      <td>Other</td>\n      <td>120.0</td>\n      <td>-0.194677</td>\n      <td>0.063026</td>\n      <td>0.125115</td>\n      <td>-0.041354</td>\n      <td>-0.122502</td>\n      <td>-0.207333</td>\n      <td>-0.042279</td>\n      <td>0.283049</td>\n      <td>...</td>\n      <td>-0.000418</td>\n      <td>-0.062899</td>\n      <td>0.048064</td>\n      <td>0.029612</td>\n      <td>0.191065</td>\n      <td>0.096081</td>\n      <td>-0.100516</td>\n      <td>-0.190299</td>\n      <td>0.224559</td>\n      <td>0.086601</td>\n    </tr>\n    <tr>\n      <th>23059</th>\n      <td>HMH Books for Young Readers</td>\n      <td>32.0</td>\n      <td>-0.115993</td>\n      <td>-0.003955</td>\n      <td>-0.027285</td>\n      <td>-0.032830</td>\n      <td>0.091905</td>\n      <td>-0.257285</td>\n      <td>-0.045624</td>\n      <td>0.374351</td>\n      <td>...</td>\n      <td>0.150964</td>\n      <td>-0.029046</td>\n      <td>0.171029</td>\n      <td>-0.072123</td>\n      <td>-0.004459</td>\n      <td>0.247430</td>\n      <td>0.111973</td>\n      <td>0.019573</td>\n      <td>0.070569</td>\n      <td>-0.112066</td>\n    </tr>\n    <tr>\n      <th>23060</th>\n      <td>Rowman &amp; Littlefield Publishers</td>\n      <td>132.0</td>\n      <td>-0.126878</td>\n      <td>-0.120418</td>\n      <td>0.198828</td>\n      <td>0.093403</td>\n      <td>-0.053232</td>\n      <td>-0.114909</td>\n      <td>-0.013179</td>\n      <td>0.289315</td>\n      <td>...</td>\n      <td>0.193755</td>\n      <td>-0.118570</td>\n      <td>0.006740</td>\n      <td>-0.108623</td>\n      <td>-0.036143</td>\n      <td>0.168113</td>\n      <td>0.136478</td>\n      <td>0.087885</td>\n      <td>0.113180</td>\n      <td>0.000569</td>\n    </tr>\n    <tr>\n      <th>23061</th>\n      <td>Other</td>\n      <td>136.0</td>\n      <td>-0.134530</td>\n      <td>-0.061256</td>\n      <td>0.178935</td>\n      <td>0.057537</td>\n      <td>-0.045066</td>\n      <td>-0.088796</td>\n      <td>-0.038559</td>\n      <td>0.219906</td>\n      <td>...</td>\n      <td>0.009007</td>\n      <td>0.154127</td>\n      <td>0.219128</td>\n      <td>-0.305824</td>\n      <td>-0.017904</td>\n      <td>-0.059886</td>\n      <td>0.108616</td>\n      <td>0.041879</td>\n      <td>-0.138893</td>\n      <td>-0.044187</td>\n    </tr>\n    <tr>\n      <th>23062</th>\n      <td>Puffin</td>\n      <td>192.0</td>\n      <td>-0.204117</td>\n      <td>-0.007189</td>\n      <td>0.375681</td>\n      <td>0.011292</td>\n      <td>-0.341423</td>\n      <td>-0.061017</td>\n      <td>0.046779</td>\n      <td>0.103854</td>\n      <td>...</td>\n      <td>0.068714</td>\n      <td>-0.065924</td>\n      <td>0.082228</td>\n      <td>-0.003849</td>\n      <td>0.099006</td>\n      <td>0.081608</td>\n      <td>0.094459</td>\n      <td>-0.048776</td>\n      <td>0.032433</td>\n      <td>0.132977</td>\n    </tr>\n  </tbody>\n</table>\n<p>22626 rows  122 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:49.451575600Z",
     "start_time": "2023-05-17T06:28:49.272973900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "       Publisher_1st World Library - Literary Society  Publisher_ADV Manga  \\\n0                                                 0.0                  0.0   \n1                                                 0.0                  0.0   \n2                                                 0.0                  0.0   \n3                                                 0.0                  0.0   \n4                                                 0.0                  0.0   \n...                                               ...                  ...   \n22621                                             0.0                  0.0   \n22622                                             0.0                  0.0   \n22623                                             0.0                  0.0   \n22624                                             0.0                  0.0   \n22625                                             0.0                  0.0   \n\n       Publisher_AMACOM/American Management Association  \\\n0                                                   0.0   \n1                                                   0.0   \n2                                                   0.0   \n3                                                   0.0   \n4                                                   0.0   \n...                                                 ...   \n22621                                               0.0   \n22622                                               0.0   \n22623                                               0.0   \n22624                                               0.0   \n22625                                               0.0   \n\n       Publisher_Abbeville Press  Publisher_Abingdon Press  Publisher_Ace  \\\n0                            0.0                       0.0            0.0   \n1                            0.0                       0.0            0.0   \n2                            0.0                       0.0            0.0   \n3                            0.0                       0.0            0.0   \n4                            0.0                       0.0            0.0   \n...                          ...                       ...            ...   \n22621                        0.0                       0.0            0.0   \n22622                        0.0                       0.0            0.0   \n22623                        0.0                       0.0            0.0   \n22624                        0.0                       0.0            0.0   \n22625                        0.0                       0.0            0.0   \n\n       Publisher_Ace Books  Publisher_Adams Media  \\\n0                      0.0                    0.0   \n1                      0.0                    0.0   \n2                      0.0                    0.0   \n3                      0.0                    0.0   \n4                      0.0                    0.0   \n...                    ...                    ...   \n22621                  0.0                    0.0   \n22622                  0.0                    0.0   \n22623                  0.0                    0.0   \n22624                  0.0                    0.0   \n22625                  0.0                    0.0   \n\n       Publisher_Addison Wesley Publishing Company  \\\n0                                              0.0   \n1                                              0.0   \n2                                              0.0   \n3                                              0.0   \n4                                              0.0   \n...                                            ...   \n22621                                          0.0   \n22622                                          0.0   \n22623                                          0.0   \n22624                                          0.0   \n22625                                          0.0   \n\n       Publisher_Addison-Wesley Professional  ...  Publisher_Yearling  \\\n0                                        0.0  ...                 0.0   \n1                                        0.0  ...                 0.0   \n2                                        0.0  ...                 0.0   \n3                                        0.0  ...                 0.0   \n4                                        0.0  ...                 0.0   \n...                                      ...  ...                 ...   \n22621                                    0.0  ...                 0.0   \n22622                                    0.0  ...                 0.0   \n22623                                    0.0  ...                 0.0   \n22624                                    0.0  ...                 0.0   \n22625                                    0.0  ...                 0.0   \n\n       Publisher_Zebra  Publisher_Zed Books  Publisher_Zonderkidz  \\\n0                  0.0                  0.0                   0.0   \n1                  0.0                  0.0                   0.0   \n2                  0.0                  0.0                   0.0   \n3                  0.0                  0.0                   0.0   \n4                  0.0                  0.0                   0.0   \n...                ...                  ...                   ...   \n22621              0.0                  0.0                   0.0   \n22622              0.0                  0.0                   0.0   \n22623              0.0                  0.0                   0.0   \n22624              0.0                  0.0                   0.0   \n22625              0.0                  0.0                   0.0   \n\n       Publisher_Zondervan  Publisher_Zondervan Academic  \\\n0                      0.0                           0.0   \n1                      0.0                           0.0   \n2                      0.0                           0.0   \n3                      0.0                           0.0   \n4                      0.0                           0.0   \n...                    ...                           ...   \n22621                  0.0                           0.0   \n22622                  0.0                           0.0   \n22623                  0.0                           0.0   \n22624                  0.0                           0.0   \n22625                  0.0                           0.0   \n\n       Publisher_Zondervan Publishing Company  Publisher_eReads.com  \\\n0                                         0.0                   0.0   \n1                                         0.0                   0.0   \n2                                         0.0                   0.0   \n3                                         0.0                   0.0   \n4                                         0.0                   0.0   \n...                                       ...                   ...   \n22621                                     0.0                   0.0   \n22622                                     0.0                   0.0   \n22623                                     0.0                   0.0   \n22624                                     0.0                   0.0   \n22625                                     0.0                   0.0   \n\n       Publisher_iBooks  Publisher_iUniverse  \n0                   0.0                  0.0  \n1                   0.0                  0.0  \n2                   0.0                  0.0  \n3                   0.0                  0.0  \n4                   0.0                  0.0  \n...                 ...                  ...  \n22621               0.0                  0.0  \n22622               0.0                  0.0  \n22623               0.0                  0.0  \n22624               0.0                  0.0  \n22625               0.0                  0.0  \n\n[22626 rows x 509 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher_1st World Library - Literary Society</th>\n      <th>Publisher_ADV Manga</th>\n      <th>Publisher_AMACOM/American Management Association</th>\n      <th>Publisher_Abbeville Press</th>\n      <th>Publisher_Abingdon Press</th>\n      <th>Publisher_Ace</th>\n      <th>Publisher_Ace Books</th>\n      <th>Publisher_Adams Media</th>\n      <th>Publisher_Addison Wesley Publishing Company</th>\n      <th>Publisher_Addison-Wesley Professional</th>\n      <th>...</th>\n      <th>Publisher_Yearling</th>\n      <th>Publisher_Zebra</th>\n      <th>Publisher_Zed Books</th>\n      <th>Publisher_Zonderkidz</th>\n      <th>Publisher_Zondervan</th>\n      <th>Publisher_Zondervan Academic</th>\n      <th>Publisher_Zondervan Publishing Company</th>\n      <th>Publisher_eReads.com</th>\n      <th>Publisher_iBooks</th>\n      <th>Publisher_iUniverse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22621</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22622</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22623</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22624</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22625</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>22626 rows  509 columns</p>\n</div>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# fit X_train\n",
    "enc.fit(X_train[['Publisher']])\n",
    "# # get feature names\n",
    "feature_names = enc.get_feature_names_out(['Publisher'])\n",
    "# # transform X_train\n",
    "X_train_ohe = enc.transform(X_train[['Publisher']])\n",
    "X_train_ohe = pd.DataFrame.sparse.from_spmatrix(X_train_ohe)\n",
    "X_train_ohe.columns = feature_names\n",
    "X_train_ohe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:58.668610900Z",
     "start_time": "2023-05-17T06:28:58.591532100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "   Publisher_1st World Library - Literary Society  Publisher_ADV Manga  \\\n0                                             0.0                  0.0   \n1                                             0.0                  0.0   \n2                                             0.0                  0.0   \n3                                             0.0                  0.0   \n4                                             0.0                  0.0   \n\n   Publisher_AMACOM/American Management Association  \\\n0                                               0.0   \n1                                               0.0   \n2                                               0.0   \n3                                               0.0   \n4                                               0.0   \n\n   Publisher_Abbeville Press  Publisher_Abingdon Press  Publisher_Ace  \\\n0                        0.0                       0.0            0.0   \n1                        0.0                       0.0            0.0   \n2                        0.0                       0.0            0.0   \n3                        0.0                       0.0            0.0   \n4                        0.0                       0.0            0.0   \n\n   Publisher_Ace Books  Publisher_Adams Media  \\\n0                  0.0                    0.0   \n1                  0.0                    0.0   \n2                  0.0                    0.0   \n3                  0.0                    0.0   \n4                  0.0                    0.0   \n\n   Publisher_Addison Wesley Publishing Company  \\\n0                                          0.0   \n1                                          0.0   \n2                                          0.0   \n3                                          0.0   \n4                                          0.0   \n\n   Publisher_Addison-Wesley Professional  ...  Publisher_Yearling  \\\n0                                    0.0  ...                 0.0   \n1                                    0.0  ...                 0.0   \n2                                    0.0  ...                 0.0   \n3                                    0.0  ...                 0.0   \n4                                    0.0  ...                 0.0   \n\n   Publisher_Zebra  Publisher_Zed Books  Publisher_Zonderkidz  \\\n0              0.0                  0.0                   0.0   \n1              0.0                  0.0                   0.0   \n2              0.0                  0.0                   0.0   \n3              0.0                  0.0                   0.0   \n4              0.0                  0.0                   0.0   \n\n   Publisher_Zondervan  Publisher_Zondervan Academic  \\\n0                  0.0                           0.0   \n1                  0.0                           0.0   \n2                  0.0                           0.0   \n3                  0.0                           0.0   \n4                  0.0                           0.0   \n\n   Publisher_Zondervan Publishing Company  Publisher_eReads.com  \\\n0                                     0.0                   0.0   \n1                                     0.0                   0.0   \n2                                     0.0                   0.0   \n3                                     0.0                   0.0   \n4                                     0.0                   0.0   \n\n   Publisher_iBooks  Publisher_iUniverse  \n0               0.0                  0.0  \n1               0.0                  0.0  \n2               0.0                  0.0  \n3               0.0                  0.0  \n4               0.0                  0.0  \n\n[5 rows x 450 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher_1st World Library - Literary Society</th>\n      <th>Publisher_ADV Manga</th>\n      <th>Publisher_AMACOM/American Management Association</th>\n      <th>Publisher_Abbeville Press</th>\n      <th>Publisher_Abingdon Press</th>\n      <th>Publisher_Ace</th>\n      <th>Publisher_Ace Books</th>\n      <th>Publisher_Adams Media</th>\n      <th>Publisher_Addison Wesley Publishing Company</th>\n      <th>Publisher_Addison-Wesley Professional</th>\n      <th>...</th>\n      <th>Publisher_Yearling</th>\n      <th>Publisher_Zebra</th>\n      <th>Publisher_Zed Books</th>\n      <th>Publisher_Zonderkidz</th>\n      <th>Publisher_Zondervan</th>\n      <th>Publisher_Zondervan Academic</th>\n      <th>Publisher_Zondervan Publishing Company</th>\n      <th>Publisher_eReads.com</th>\n      <th>Publisher_iBooks</th>\n      <th>Publisher_iUniverse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  450 columns</p>\n</div>"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "mi = SelectKBest(mutual_info_classif, k=450)\n",
    "X_train_ohe_mi = mi.fit_transform(X_train_ohe, y_train)\n",
    "X_train_ohe_mi = pd.DataFrame.sparse.from_spmatrix(X_train_ohe_mi)\n",
    "X_train_ohe_mi.columns = X_train_ohe.columns[mi.get_support()]\n",
    "X_train_ohe_mi.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:29:02.700463300Z",
     "start_time": "2023-05-17T06:29:01.014933100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "       Publisher_1st World Library - Literary Society  Publisher_ADV Manga  \\\n0                                                 0.0                  0.0   \n1                                                 0.0                  0.0   \n2                                                 0.0                  0.0   \n3                                                 0.0                  0.0   \n4                                                 0.0                  0.0   \n...                                               ...                  ...   \n22621                                             0.0                  0.0   \n22622                                             0.0                  0.0   \n22623                                             0.0                  0.0   \n22624                                             0.0                  0.0   \n22625                                             0.0                  0.0   \n\n       Publisher_AMACOM/American Management Association  \\\n0                                                   0.0   \n1                                                   0.0   \n2                                                   0.0   \n3                                                   0.0   \n4                                                   0.0   \n...                                                 ...   \n22621                                               0.0   \n22622                                               0.0   \n22623                                               0.0   \n22624                                               0.0   \n22625                                               0.0   \n\n       Publisher_Abbeville Press  Publisher_Abingdon Press  Publisher_Ace  \\\n0                            0.0                       0.0            0.0   \n1                            0.0                       0.0            0.0   \n2                            0.0                       0.0            0.0   \n3                            0.0                       0.0            0.0   \n4                            0.0                       0.0            0.0   \n...                          ...                       ...            ...   \n22621                        0.0                       0.0            0.0   \n22622                        0.0                       0.0            0.0   \n22623                        0.0                       0.0            0.0   \n22624                        0.0                       0.0            0.0   \n22625                        0.0                       0.0            0.0   \n\n       Publisher_Ace Books  Publisher_Adams Media  \\\n0                      0.0                    0.0   \n1                      0.0                    0.0   \n2                      0.0                    0.0   \n3                      0.0                    0.0   \n4                      0.0                    0.0   \n...                    ...                    ...   \n22621                  0.0                    0.0   \n22622                  0.0                    0.0   \n22623                  0.0                    0.0   \n22624                  0.0                    0.0   \n22625                  0.0                    0.0   \n\n       Publisher_Addison Wesley Publishing Company  \\\n0                                              0.0   \n1                                              0.0   \n2                                              0.0   \n3                                              0.0   \n4                                              0.0   \n...                                            ...   \n22621                                          0.0   \n22622                                          0.0   \n22623                                          0.0   \n22624                                          0.0   \n22625                                          0.0   \n\n       Publisher_Addison-Wesley Professional  ...   Name_90   Name_91  \\\n0                                        0.0  ... -0.172811  0.098389   \n1                                        0.0  ...  0.245650 -0.049657   \n2                                        0.0  ... -0.033781  0.093943   \n3                                        0.0  ...  0.020762 -0.149720   \n4                                        0.0  ...  0.191644  0.044182   \n...                                      ...  ...       ...       ...   \n22621                                    0.0  ... -0.000418 -0.062899   \n22622                                    0.0  ...  0.150964 -0.029046   \n22623                                    0.0  ...  0.193755 -0.118570   \n22624                                    0.0  ...  0.009007  0.154127   \n22625                                    0.0  ...  0.068714 -0.065924   \n\n        Name_92   Name_93   Name_94   Name_95   Name_96   Name_97   Name_98  \\\n0     -0.062941  0.118057 -0.065377  0.227973  0.218879 -0.151266 -0.048105   \n1      0.072740 -0.055925 -0.000046  0.140500  0.067133 -0.238091  0.109774   \n2      0.132654  0.030295  0.102714  0.154334  0.129325 -0.231493  0.007541   \n3      0.150557  0.294355  0.001157  0.285179  0.049340 -0.037548  0.042920   \n4      0.054631 -0.025782  0.049917  0.122052 -0.084216 -0.096424 -0.068681   \n...         ...       ...       ...       ...       ...       ...       ...   \n22621  0.048064  0.029612  0.191065  0.096081 -0.100516 -0.190299  0.224559   \n22622  0.171029 -0.072123 -0.004459  0.247430  0.111973  0.019573  0.070569   \n22623  0.006740 -0.108623 -0.036143  0.168113  0.136478  0.087885  0.113180   \n22624  0.219128 -0.305824 -0.017904 -0.059886  0.108616  0.041879 -0.138893   \n22625  0.082228 -0.003849  0.099006  0.081608  0.094459 -0.048776  0.032433   \n\n        Name_99  \n0      0.300822  \n1     -0.156772  \n2     -0.098540  \n3      0.176173  \n4     -0.005293  \n...         ...  \n22621  0.086601  \n22622 -0.112066  \n22623  0.000569  \n22624 -0.044187  \n22625  0.132977  \n\n[22626 rows x 571 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher_1st World Library - Literary Society</th>\n      <th>Publisher_ADV Manga</th>\n      <th>Publisher_AMACOM/American Management Association</th>\n      <th>Publisher_Abbeville Press</th>\n      <th>Publisher_Abingdon Press</th>\n      <th>Publisher_Ace</th>\n      <th>Publisher_Ace Books</th>\n      <th>Publisher_Adams Media</th>\n      <th>Publisher_Addison Wesley Publishing Company</th>\n      <th>Publisher_Addison-Wesley Professional</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22621</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.000418</td>\n      <td>-0.062899</td>\n      <td>0.048064</td>\n      <td>0.029612</td>\n      <td>0.191065</td>\n      <td>0.096081</td>\n      <td>-0.100516</td>\n      <td>-0.190299</td>\n      <td>0.224559</td>\n      <td>0.086601</td>\n    </tr>\n    <tr>\n      <th>22622</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.150964</td>\n      <td>-0.029046</td>\n      <td>0.171029</td>\n      <td>-0.072123</td>\n      <td>-0.004459</td>\n      <td>0.247430</td>\n      <td>0.111973</td>\n      <td>0.019573</td>\n      <td>0.070569</td>\n      <td>-0.112066</td>\n    </tr>\n    <tr>\n      <th>22623</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.193755</td>\n      <td>-0.118570</td>\n      <td>0.006740</td>\n      <td>-0.108623</td>\n      <td>-0.036143</td>\n      <td>0.168113</td>\n      <td>0.136478</td>\n      <td>0.087885</td>\n      <td>0.113180</td>\n      <td>0.000569</td>\n    </tr>\n    <tr>\n      <th>22624</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.009007</td>\n      <td>0.154127</td>\n      <td>0.219128</td>\n      <td>-0.305824</td>\n      <td>-0.017904</td>\n      <td>-0.059886</td>\n      <td>0.108616</td>\n      <td>0.041879</td>\n      <td>-0.138893</td>\n      <td>-0.044187</td>\n    </tr>\n    <tr>\n      <th>22625</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.068714</td>\n      <td>-0.065924</td>\n      <td>0.082228</td>\n      <td>-0.003849</td>\n      <td>0.099006</td>\n      <td>0.081608</td>\n      <td>0.094459</td>\n      <td>-0.048776</td>\n      <td>0.032433</td>\n      <td>0.132977</td>\n    </tr>\n  </tbody>\n</table>\n<p>22626 rows  571 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat pages number\n",
    "X_train_ohe_mi = pd.concat([X_train_ohe_mi.reset_index(drop=True), X_train.drop(columns=['Publisher'], axis=1).reset_index(drop=True)], axis=1)\n",
    "X_train_ohe_mi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:29:07.224578800Z",
     "start_time": "2023-05-17T06:29:07.066561200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7026871775841486, precision: 0.4937692784677183, recall: 0.7026871775841486, f1: 0.5799882434316336\n",
      "accuracy_sd: 9.448037681400046e-05, precision_sd: 0.00013277536755249437, recall_sd: 9.448037681400046e-05, f1_sd: 0.00012378062766390402\n",
      "training time: 0.40660700798034666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# build a 0r model and evaluate its performance using cross validation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import time\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "start = time.time()\n",
    "cv_results = cross_validate(dummy_clf, X_train_ohe_mi, y_train, scoring=scoring, cv=5, return_train_score=True)\n",
    "end = time.time()\n",
    "print(f\"accuracy: {cv_results['test_accuracy'].mean()}, precision: {cv_results['test_precision_weighted'].mean()}, recall: {cv_results['test_recall_weighted'].mean()}, f1: {cv_results['test_f1_weighted'].mean()}\")\n",
    "print(f\"accuracy_sd: {cv_results['test_accuracy'].std()}, precision_sd: {cv_results['test_precision_weighted'].std()}, recall_sd: {cv_results['test_recall_weighted'].std()}, f1_sd: {cv_results['test_f1_weighted'].std()}\")\n",
    "print(f\"training time: {(end - start)/5}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:46:26.105739200Z",
     "start_time": "2023-05-16T15:46:24.043651900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0       1.256406      0.414037        34.547310       10.845633   \n1       2.900122      1.224969        60.737560       17.284331   \n2       4.708604      0.178971        67.425809       11.167485   \n3       2.203485      0.944905        41.471444       15.535885   \n4       2.734648      1.383573        31.085523        8.647673   \n5       1.695712      0.088056        27.023908        9.230987   \n6       3.028000      0.923209        33.441916        9.438247   \n7       2.679418      0.671844        24.640843        4.328415   \n8       2.939721      0.815416        20.115019        1.427760   \n\n  param_n_neighbors               params  split0_test_accuracy  \\\n0                 3   {'n_neighbors': 3}              0.619311   \n1                 5   {'n_neighbors': 5}              0.640521   \n2                 9   {'n_neighbors': 9}              0.666814   \n3                15  {'n_neighbors': 15}              0.689792   \n4                21  {'n_neighbors': 21}              0.694653   \n5                25  {'n_neighbors': 25}              0.696421   \n6                29  {'n_neighbors': 29}              0.699514   \n7                31  {'n_neighbors': 31}              0.700177   \n8                35  {'n_neighbors': 35}              0.699956   \n\n   split1_test_accuracy  split2_test_accuracy  split3_test_accuracy  ...  \\\n0              0.627624              0.632265              0.631823  ...   \n1              0.649503              0.653923              0.650166  ...   \n2              0.673149              0.674033              0.668729  ...   \n3              0.686409              0.687956              0.683094  ...   \n4              0.694365              0.694807              0.692818  ...   \n5              0.699448              0.695912              0.696796  ...   \n6              0.697017              0.697901              0.700110  ...   \n7              0.699227              0.697459              0.703204  ...   \n8              0.700994              0.696796              0.702762  ...   \n\n   std_test_recall_weighted  rank_test_recall_weighted  \\\n0                  0.006767                          9   \n1                  0.004870                          8   \n2                  0.002760                          7   \n3                  0.003234                          6   \n4                  0.001993                          5   \n5                  0.001652                          4   \n6                  0.001615                          3   \n7                  0.002216                          2   \n8                  0.002215                          1   \n\n   split0_test_f1_weighted  split1_test_f1_weighted  split2_test_f1_weighted  \\\n0                 0.598785                 0.608749                 0.610940   \n1                 0.601955                 0.613073                 0.617129   \n2                 0.603915                 0.611228                 0.615363   \n3                 0.609181                 0.602870                 0.608052   \n4                 0.596701                 0.596069                 0.600754   \n5                 0.592939                 0.597188                 0.596120   \n6                 0.591500                 0.591319                 0.594225   \n7                 0.589924                 0.591467                 0.590088   \n8                 0.586168                 0.590981                 0.587878   \n\n   split3_test_f1_weighted  split4_test_f1_weighted  mean_test_f1_weighted  \\\n0                 0.609242                 0.596825               0.604908   \n1                 0.614161                 0.607759               0.610815   \n2                 0.613733                 0.617075               0.612263   \n3                 0.606569                 0.605068               0.606348   \n4                 0.605391                 0.597934               0.599370   \n5                 0.601043                 0.597608               0.596980   \n6                 0.601851                 0.595441               0.594867   \n7                 0.602005                 0.595860               0.593869   \n8                 0.597972                 0.592311               0.591062   \n\n   std_test_f1_weighted  rank_test_f1_weighted  \n0              0.005878                      4  \n1              0.005366                      2  \n2              0.004598                      1  \n3              0.002223                      3  \n4              0.003413                      5  \n5              0.002609                      6  \n6              0.003832                      7  \n7              0.004600                      8  \n8              0.004084                      9  \n\n[9 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_neighbors</th>\n      <th>params</th>\n      <th>split0_test_accuracy</th>\n      <th>split1_test_accuracy</th>\n      <th>split2_test_accuracy</th>\n      <th>split3_test_accuracy</th>\n      <th>...</th>\n      <th>std_test_recall_weighted</th>\n      <th>rank_test_recall_weighted</th>\n      <th>split0_test_f1_weighted</th>\n      <th>split1_test_f1_weighted</th>\n      <th>split2_test_f1_weighted</th>\n      <th>split3_test_f1_weighted</th>\n      <th>split4_test_f1_weighted</th>\n      <th>mean_test_f1_weighted</th>\n      <th>std_test_f1_weighted</th>\n      <th>rank_test_f1_weighted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.256406</td>\n      <td>0.414037</td>\n      <td>34.547310</td>\n      <td>10.845633</td>\n      <td>3</td>\n      <td>{'n_neighbors': 3}</td>\n      <td>0.619311</td>\n      <td>0.627624</td>\n      <td>0.632265</td>\n      <td>0.631823</td>\n      <td>...</td>\n      <td>0.006767</td>\n      <td>9</td>\n      <td>0.598785</td>\n      <td>0.608749</td>\n      <td>0.610940</td>\n      <td>0.609242</td>\n      <td>0.596825</td>\n      <td>0.604908</td>\n      <td>0.005878</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.900122</td>\n      <td>1.224969</td>\n      <td>60.737560</td>\n      <td>17.284331</td>\n      <td>5</td>\n      <td>{'n_neighbors': 5}</td>\n      <td>0.640521</td>\n      <td>0.649503</td>\n      <td>0.653923</td>\n      <td>0.650166</td>\n      <td>...</td>\n      <td>0.004870</td>\n      <td>8</td>\n      <td>0.601955</td>\n      <td>0.613073</td>\n      <td>0.617129</td>\n      <td>0.614161</td>\n      <td>0.607759</td>\n      <td>0.610815</td>\n      <td>0.005366</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.708604</td>\n      <td>0.178971</td>\n      <td>67.425809</td>\n      <td>11.167485</td>\n      <td>9</td>\n      <td>{'n_neighbors': 9}</td>\n      <td>0.666814</td>\n      <td>0.673149</td>\n      <td>0.674033</td>\n      <td>0.668729</td>\n      <td>...</td>\n      <td>0.002760</td>\n      <td>7</td>\n      <td>0.603915</td>\n      <td>0.611228</td>\n      <td>0.615363</td>\n      <td>0.613733</td>\n      <td>0.617075</td>\n      <td>0.612263</td>\n      <td>0.004598</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.203485</td>\n      <td>0.944905</td>\n      <td>41.471444</td>\n      <td>15.535885</td>\n      <td>15</td>\n      <td>{'n_neighbors': 15}</td>\n      <td>0.689792</td>\n      <td>0.686409</td>\n      <td>0.687956</td>\n      <td>0.683094</td>\n      <td>...</td>\n      <td>0.003234</td>\n      <td>6</td>\n      <td>0.609181</td>\n      <td>0.602870</td>\n      <td>0.608052</td>\n      <td>0.606569</td>\n      <td>0.605068</td>\n      <td>0.606348</td>\n      <td>0.002223</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.734648</td>\n      <td>1.383573</td>\n      <td>31.085523</td>\n      <td>8.647673</td>\n      <td>21</td>\n      <td>{'n_neighbors': 21}</td>\n      <td>0.694653</td>\n      <td>0.694365</td>\n      <td>0.694807</td>\n      <td>0.692818</td>\n      <td>...</td>\n      <td>0.001993</td>\n      <td>5</td>\n      <td>0.596701</td>\n      <td>0.596069</td>\n      <td>0.600754</td>\n      <td>0.605391</td>\n      <td>0.597934</td>\n      <td>0.599370</td>\n      <td>0.003413</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.695712</td>\n      <td>0.088056</td>\n      <td>27.023908</td>\n      <td>9.230987</td>\n      <td>25</td>\n      <td>{'n_neighbors': 25}</td>\n      <td>0.696421</td>\n      <td>0.699448</td>\n      <td>0.695912</td>\n      <td>0.696796</td>\n      <td>...</td>\n      <td>0.001652</td>\n      <td>4</td>\n      <td>0.592939</td>\n      <td>0.597188</td>\n      <td>0.596120</td>\n      <td>0.601043</td>\n      <td>0.597608</td>\n      <td>0.596980</td>\n      <td>0.002609</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3.028000</td>\n      <td>0.923209</td>\n      <td>33.441916</td>\n      <td>9.438247</td>\n      <td>29</td>\n      <td>{'n_neighbors': 29}</td>\n      <td>0.699514</td>\n      <td>0.697017</td>\n      <td>0.697901</td>\n      <td>0.700110</td>\n      <td>...</td>\n      <td>0.001615</td>\n      <td>3</td>\n      <td>0.591500</td>\n      <td>0.591319</td>\n      <td>0.594225</td>\n      <td>0.601851</td>\n      <td>0.595441</td>\n      <td>0.594867</td>\n      <td>0.003832</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.679418</td>\n      <td>0.671844</td>\n      <td>24.640843</td>\n      <td>4.328415</td>\n      <td>31</td>\n      <td>{'n_neighbors': 31}</td>\n      <td>0.700177</td>\n      <td>0.699227</td>\n      <td>0.697459</td>\n      <td>0.703204</td>\n      <td>...</td>\n      <td>0.002216</td>\n      <td>2</td>\n      <td>0.589924</td>\n      <td>0.591467</td>\n      <td>0.590088</td>\n      <td>0.602005</td>\n      <td>0.595860</td>\n      <td>0.593869</td>\n      <td>0.004600</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.939721</td>\n      <td>0.815416</td>\n      <td>20.115019</td>\n      <td>1.427760</td>\n      <td>35</td>\n      <td>{'n_neighbors': 35}</td>\n      <td>0.699956</td>\n      <td>0.700994</td>\n      <td>0.696796</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.002215</td>\n      <td>1</td>\n      <td>0.586168</td>\n      <td>0.590981</td>\n      <td>0.587878</td>\n      <td>0.597972</td>\n      <td>0.592311</td>\n      <td>0.591062</td>\n      <td>0.004084</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>9 rows  38 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 9, 15, 21, 25, 29, 31, 35]\n",
    "}\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], refit=False, n_jobs=-1)\n",
    "grid_search.fit(X_train_ohe_mi, y_train)\n",
    "search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "search_results.to_csv(r\"knn_grid_search_results.csv\", index=False)\n",
    "search_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:48:43.140820200Z",
     "start_time": "2023-05-16T15:46:32.953727100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        1.408249      0.033924         0.079801        0.025326   \n1        1.554126      0.060768         0.076623        0.009517   \n2        1.881481      0.087703         0.082798        0.008921   \n3        3.761954      0.188945         0.083999        0.011611   \n4        6.458592      0.333957         0.132694        0.032108   \n5       13.073194      1.359581         0.183655        0.014921   \n6       17.556915      0.227446         0.259997        0.035528   \n7        1.220924      0.111536         0.068415        0.015321   \n8        2.207090      0.604433         0.088020        0.056639   \n9        2.732533      0.430209         0.095312        0.018309   \n10       6.513281      0.355321         0.117932        0.005480   \n11      12.563625      0.611457         0.186201        0.006912   \n12      23.777351      1.339645         0.309892        0.012089   \n13      35.656549      1.933852         0.442049        0.015196   \n14       1.297402      0.064220         0.067397        0.007868   \n15       1.652322      0.086871         0.073771        0.011680   \n16       2.617353      0.096881         0.084311        0.010682   \n17       9.065110      0.445583         0.159000        0.018188   \n18      17.957699      0.952308         0.264194        0.028548   \n19      34.530249      0.917709         0.473735        0.022461   \n20      51.818509      1.984330         0.723452        0.152528   \n21       1.516421      0.084736         0.069156        0.006050   \n22       2.016819      0.109660         0.079316        0.015404   \n23       3.094194      0.086147         0.081997        0.002285   \n24      11.446477      0.286786         0.192669        0.032974   \n25      22.080496      1.056220         0.300064        0.037323   \n26      42.728838      2.159839         0.535035        0.031863   \n27      65.911409      3.546866         0.841862        0.076846   \n28       1.617096      0.100042         0.068514        0.007319   \n29       2.360732      0.312713         0.103816        0.012775   \n30       3.223116      0.144879         0.091914        0.009295   \n31      12.785458      0.816343         0.196002        0.019606   \n32      24.348558      1.302333         0.313023        0.015366   \n33      48.907190      3.604454         0.576040        0.017595   \n34      70.561986      3.151758         0.750519        0.042283   \n35       1.735697      0.103134         0.068800        0.005705   \n36       2.123360      0.090228         0.078598        0.010842   \n37       3.396755      0.233423         0.094752        0.018988   \n38      14.091371      1.477842         0.195708        0.008198   \n39      25.329111      1.105834         0.333201        0.006652   \n40      45.932872      0.813640         0.435839        0.028102   \n41      55.486510      2.852526         0.533100        0.042226   \n\n   param_max_depth param_n_estimators                                  params  \\\n0                5                  3     {'max_depth': 5, 'n_estimators': 3}   \n1                5                  5     {'max_depth': 5, 'n_estimators': 5}   \n2                5                 10    {'max_depth': 5, 'n_estimators': 10}   \n3                5                 50    {'max_depth': 5, 'n_estimators': 50}   \n4                5                100   {'max_depth': 5, 'n_estimators': 100}   \n5                5                200   {'max_depth': 5, 'n_estimators': 200}   \n6                5                300   {'max_depth': 5, 'n_estimators': 300}   \n7               10                  3    {'max_depth': 10, 'n_estimators': 3}   \n8               10                  5    {'max_depth': 10, 'n_estimators': 5}   \n9               10                 10   {'max_depth': 10, 'n_estimators': 10}   \n10              10                 50   {'max_depth': 10, 'n_estimators': 50}   \n11              10                100  {'max_depth': 10, 'n_estimators': 100}   \n12              10                200  {'max_depth': 10, 'n_estimators': 200}   \n13              10                300  {'max_depth': 10, 'n_estimators': 300}   \n14              15                  3    {'max_depth': 15, 'n_estimators': 3}   \n15              15                  5    {'max_depth': 15, 'n_estimators': 5}   \n16              15                 10   {'max_depth': 15, 'n_estimators': 10}   \n17              15                 50   {'max_depth': 15, 'n_estimators': 50}   \n18              15                100  {'max_depth': 15, 'n_estimators': 100}   \n19              15                200  {'max_depth': 15, 'n_estimators': 200}   \n20              15                300  {'max_depth': 15, 'n_estimators': 300}   \n21              20                  3    {'max_depth': 20, 'n_estimators': 3}   \n22              20                  5    {'max_depth': 20, 'n_estimators': 5}   \n23              20                 10   {'max_depth': 20, 'n_estimators': 10}   \n24              20                 50   {'max_depth': 20, 'n_estimators': 50}   \n25              20                100  {'max_depth': 20, 'n_estimators': 100}   \n26              20                200  {'max_depth': 20, 'n_estimators': 200}   \n27              20                300  {'max_depth': 20, 'n_estimators': 300}   \n28              25                  3    {'max_depth': 25, 'n_estimators': 3}   \n29              25                  5    {'max_depth': 25, 'n_estimators': 5}   \n30              25                 10   {'max_depth': 25, 'n_estimators': 10}   \n31              25                 50   {'max_depth': 25, 'n_estimators': 50}   \n32              25                100  {'max_depth': 25, 'n_estimators': 100}   \n33              25                200  {'max_depth': 25, 'n_estimators': 200}   \n34              25                300  {'max_depth': 25, 'n_estimators': 300}   \n35              30                  3    {'max_depth': 30, 'n_estimators': 3}   \n36              30                  5    {'max_depth': 30, 'n_estimators': 5}   \n37              30                 10   {'max_depth': 30, 'n_estimators': 10}   \n38              30                 50   {'max_depth': 30, 'n_estimators': 50}   \n39              30                100  {'max_depth': 30, 'n_estimators': 100}   \n40              30                200  {'max_depth': 30, 'n_estimators': 200}   \n41              30                300  {'max_depth': 30, 'n_estimators': 300}   \n\n    split0_test_accuracy  split1_test_accuracy  split2_test_accuracy  ...  \\\n0               0.701723              0.702320              0.704530  ...   \n1               0.702386              0.701878              0.702541  ...   \n2               0.703049              0.702983              0.702762  ...   \n3               0.702607              0.702762              0.702762  ...   \n4               0.702607              0.702762              0.702762  ...   \n5               0.702607              0.702762              0.702762  ...   \n6               0.702607              0.702762              0.702762  ...   \n7               0.690455              0.695912              0.687735  ...   \n8               0.698409              0.699448              0.699448  ...   \n9               0.700619              0.702983              0.702541  ...   \n10              0.702607              0.702762              0.702762  ...   \n11              0.702607              0.702983              0.702762  ...   \n12              0.702607              0.702762              0.702762  ...   \n13              0.702607              0.702762              0.702762  ...   \n14              0.667035              0.673370              0.675138  ...   \n15              0.690897              0.690166              0.687072  ...   \n16              0.694874              0.700110              0.701657  ...   \n17              0.703270              0.702762              0.702983  ...   \n18              0.702386              0.702762              0.702983  ...   \n19              0.702607              0.702762              0.702762  ...   \n20              0.702607              0.702762              0.702983  ...   \n21              0.646929              0.650387              0.636243  ...   \n22              0.665488              0.662762              0.662983  ...   \n23              0.684490              0.687072              0.689724  ...   \n24              0.701502              0.701878              0.703646  ...   \n25              0.701502              0.701657              0.702983  ...   \n26              0.702607              0.702320              0.704088  ...   \n27              0.702607              0.702983              0.703425  ...   \n28              0.620415              0.622541              0.626077  ...   \n29              0.640521              0.645083              0.640000  ...   \n30              0.666372              0.676685              0.680442  ...   \n31              0.700619              0.702099              0.702983  ...   \n32              0.701723              0.702099              0.702762  ...   \n33              0.702386              0.702983              0.704309  ...   \n34              0.703049              0.703425              0.702762  ...   \n35              0.621078              0.615028              0.619448  ...   \n36              0.629253              0.629171              0.641326  ...   \n37              0.662837              0.655028              0.665856  ...   \n38              0.699072              0.701878              0.704088  ...   \n39              0.701502              0.702762              0.703867  ...   \n40              0.703049              0.703646              0.703867  ...   \n41              0.703491              0.703204              0.703646  ...   \n\n    std_test_recall_weighted  rank_test_recall_weighted  \\\n0                   0.001144                          2   \n1                   0.000297                         23   \n2                   0.000181                         10   \n3                   0.000094                         13   \n4                   0.000094                         13   \n5                   0.000094                         13   \n6                   0.000094                         13   \n7                   0.003708                         31   \n8                   0.000630                         29   \n9                   0.000799                         25   \n10                  0.000094                         13   \n11                  0.000153                         11   \n12                  0.000094                         13   \n13                  0.000094                         13   \n14                  0.002985                         35   \n15                  0.001727                         32   \n16                  0.002279                         30   \n17                  0.000213                          5   \n18                  0.000251                         22   \n19                  0.000094                         13   \n20                  0.000153                         11   \n21                  0.009766                         38   \n22                  0.001549                         36   \n23                  0.002226                         33   \n24                  0.001126                         26   \n25                  0.000627                         24   \n26                  0.000657                          9   \n27                  0.000376                          8   \n28                  0.003283                         41   \n29                  0.003876                         39   \n30                  0.005074                         34   \n31                  0.001083                         27   \n32                  0.000675                         21   \n33                  0.000679                          3   \n34                  0.000296                          7   \n35                  0.008529                         42   \n36                  0.007835                         40   \n37                  0.004628                         37   \n38                  0.001651                         28   \n39                  0.000877                          6   \n40                  0.000347                          1   \n41                  0.000466                          4   \n\n    split0_test_f1_weighted  split1_test_f1_weighted  split2_test_f1_weighted  \\\n0                  0.580747                 0.582301                 0.585001   \n1                  0.580186                 0.579658                 0.580389   \n2                  0.580916                 0.580604                 0.580087   \n3                  0.579883                 0.580087                 0.580087   \n4                  0.579883                 0.580087                 0.580087   \n5                  0.579883                 0.580087                 0.580087   \n6                  0.579883                 0.580087                 0.580087   \n7                  0.589156                 0.594429                 0.588857   \n8                  0.582252                 0.582587                 0.582832   \n9                  0.580532                 0.581419                 0.582009   \n10                 0.579883                 0.580087                 0.580087   \n11                 0.579883                 0.580604                 0.580087   \n12                 0.579883                 0.580087                 0.580087   \n13                 0.579883                 0.580087                 0.580087   \n14                 0.600619                 0.591969                 0.601001   \n15                 0.598027                 0.593211                 0.595335   \n16                 0.587992                 0.586681                 0.592849   \n17                 0.581837                 0.580087                 0.580604   \n18                 0.579776                 0.580087                 0.580604   \n19                 0.579883                 0.580087                 0.580087   \n20                 0.579883                 0.580087                 0.580604   \n21                 0.600946                 0.601770                 0.598442   \n22                 0.597100                 0.595723                 0.604110   \n23                 0.599097                 0.591657                 0.604104   \n24                 0.581369                 0.583023                 0.584557   \n25                 0.579905                 0.579958                 0.581824   \n26                 0.580293                 0.579873                 0.583177   \n27                 0.580293                 0.580604                 0.582041   \n28                 0.597626                 0.596128                 0.598125   \n29                 0.588993                 0.593984                 0.596505   \n30                 0.595260                 0.606527                 0.599791   \n31                 0.583830                 0.586327                 0.586253   \n32                 0.580418                 0.581389                 0.582118   \n33                 0.580185                 0.581824                 0.584890   \n34                 0.580991                 0.582041                 0.581310   \n35                 0.598605                 0.592918                 0.592053   \n36                 0.595096                 0.593417                 0.600902   \n37                 0.607125                 0.590571                 0.607030   \n38                 0.585024                 0.589106                 0.593243   \n39                 0.581442                 0.584507                 0.588183   \n40                 0.581729                 0.582555                 0.584270   \n41                 0.581946                 0.582337                 0.583360   \n\n    split3_test_f1_weighted  split4_test_f1_weighted  mean_test_f1_weighted  \\\n0                  0.585073                 0.585687               0.583762   \n1                  0.580496                 0.579797               0.580105   \n2                  0.580087                 0.579797               0.580298   \n3                  0.580087                 0.579797               0.579988   \n4                  0.580087                 0.579797               0.579988   \n5                  0.580087                 0.579797               0.579988   \n6                  0.580087                 0.579797               0.579988   \n7                  0.584317                 0.589696               0.589291   \n8                  0.582777                 0.582886               0.582667   \n9                  0.581864                 0.581501               0.581465   \n10                 0.580087                 0.579797               0.579988   \n11                 0.580087                 0.579797               0.580092   \n12                 0.580087                 0.579797               0.579988   \n13                 0.580087                 0.579797               0.579988   \n14                 0.594990                 0.604648               0.598646   \n15                 0.595755                 0.600106               0.596487   \n16                 0.587665                 0.586961               0.588430   \n17                 0.581527                 0.580722               0.580955   \n18                 0.580087                 0.579690               0.580049   \n19                 0.580087                 0.579797               0.579988   \n20                 0.580087                 0.579797               0.580092   \n21                 0.594766                 0.605648               0.600314   \n22                 0.604445                 0.594261               0.599128   \n23                 0.595233                 0.593513               0.596721   \n24                 0.583319                 0.579608               0.582375   \n25                 0.580496                 0.579776               0.580392   \n26                 0.580087                 0.579690               0.580624   \n27                 0.580604                 0.579690               0.580646   \n28                 0.600043                 0.591843               0.596753   \n29                 0.593245                 0.599339               0.594413   \n30                 0.592553                 0.598654               0.598557   \n31                 0.584284                 0.582208               0.584580   \n32                 0.582958                 0.582851               0.581947   \n33                 0.581120                 0.580614               0.581727   \n34                 0.579980                 0.580830               0.581030   \n35                 0.582059                 0.588421               0.590811   \n36                 0.597264                 0.610628               0.599462   \n37                 0.601675                 0.604066               0.602094   \n38                 0.586338                 0.587320               0.588206   \n39                 0.583319                 0.585675               0.584625   \n40                 0.581527                 0.581643               0.582345   \n41                 0.580389                 0.580207               0.581648   \n\n    std_test_f1_weighted  rank_test_f1_weighted  \n0               0.001907                     17  \n1               0.000327                     31  \n2               0.000404                     30  \n3               0.000124                     35  \n4               0.000124                     35  \n5               0.000124                     35  \n6               0.000124                     35  \n7               0.003210                     12  \n8               0.000231                     18  \n9               0.000516                     24  \n10              0.000124                     35  \n11              0.000280                     32  \n12              0.000124                     35  \n13              0.000124                     35  \n14              0.004548                      5  \n15              0.002369                      9  \n16              0.002259                     13  \n17              0.000638                     26  \n18              0.000321                     34  \n19              0.000124                     35  \n20              0.000280                     32  \n21              0.003613                      2  \n22              0.004301                      4  \n23              0.004434                      8  \n24              0.001717                     19  \n25              0.000757                     29  \n26              0.001293                     28  \n27              0.000773                     27  \n28              0.002756                      7  \n29              0.003452                     10  \n30              0.004732                      6  \n31              0.001557                     16  \n32              0.000951                     21  \n33              0.001673                     22  \n34              0.000670                     25  \n35              0.005460                     11  \n36              0.006119                      3  \n37              0.006108                      1  \n38              0.002849                     14  \n39              0.002263                     15  \n40              0.001029                     20  \n41              0.001197                     23  \n\n[42 rows x 39 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_max_depth</th>\n      <th>param_n_estimators</th>\n      <th>params</th>\n      <th>split0_test_accuracy</th>\n      <th>split1_test_accuracy</th>\n      <th>split2_test_accuracy</th>\n      <th>...</th>\n      <th>std_test_recall_weighted</th>\n      <th>rank_test_recall_weighted</th>\n      <th>split0_test_f1_weighted</th>\n      <th>split1_test_f1_weighted</th>\n      <th>split2_test_f1_weighted</th>\n      <th>split3_test_f1_weighted</th>\n      <th>split4_test_f1_weighted</th>\n      <th>mean_test_f1_weighted</th>\n      <th>std_test_f1_weighted</th>\n      <th>rank_test_f1_weighted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.408249</td>\n      <td>0.033924</td>\n      <td>0.079801</td>\n      <td>0.025326</td>\n      <td>5</td>\n      <td>3</td>\n      <td>{'max_depth': 5, 'n_estimators': 3}</td>\n      <td>0.701723</td>\n      <td>0.702320</td>\n      <td>0.704530</td>\n      <td>...</td>\n      <td>0.001144</td>\n      <td>2</td>\n      <td>0.580747</td>\n      <td>0.582301</td>\n      <td>0.585001</td>\n      <td>0.585073</td>\n      <td>0.585687</td>\n      <td>0.583762</td>\n      <td>0.001907</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.554126</td>\n      <td>0.060768</td>\n      <td>0.076623</td>\n      <td>0.009517</td>\n      <td>5</td>\n      <td>5</td>\n      <td>{'max_depth': 5, 'n_estimators': 5}</td>\n      <td>0.702386</td>\n      <td>0.701878</td>\n      <td>0.702541</td>\n      <td>...</td>\n      <td>0.000297</td>\n      <td>23</td>\n      <td>0.580186</td>\n      <td>0.579658</td>\n      <td>0.580389</td>\n      <td>0.580496</td>\n      <td>0.579797</td>\n      <td>0.580105</td>\n      <td>0.000327</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.881481</td>\n      <td>0.087703</td>\n      <td>0.082798</td>\n      <td>0.008921</td>\n      <td>5</td>\n      <td>10</td>\n      <td>{'max_depth': 5, 'n_estimators': 10}</td>\n      <td>0.703049</td>\n      <td>0.702983</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000181</td>\n      <td>10</td>\n      <td>0.580916</td>\n      <td>0.580604</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.580298</td>\n      <td>0.000404</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.761954</td>\n      <td>0.188945</td>\n      <td>0.083999</td>\n      <td>0.011611</td>\n      <td>5</td>\n      <td>50</td>\n      <td>{'max_depth': 5, 'n_estimators': 50}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.458592</td>\n      <td>0.333957</td>\n      <td>0.132694</td>\n      <td>0.032108</td>\n      <td>5</td>\n      <td>100</td>\n      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>13.073194</td>\n      <td>1.359581</td>\n      <td>0.183655</td>\n      <td>0.014921</td>\n      <td>5</td>\n      <td>200</td>\n      <td>{'max_depth': 5, 'n_estimators': 200}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>17.556915</td>\n      <td>0.227446</td>\n      <td>0.259997</td>\n      <td>0.035528</td>\n      <td>5</td>\n      <td>300</td>\n      <td>{'max_depth': 5, 'n_estimators': 300}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.220924</td>\n      <td>0.111536</td>\n      <td>0.068415</td>\n      <td>0.015321</td>\n      <td>10</td>\n      <td>3</td>\n      <td>{'max_depth': 10, 'n_estimators': 3}</td>\n      <td>0.690455</td>\n      <td>0.695912</td>\n      <td>0.687735</td>\n      <td>...</td>\n      <td>0.003708</td>\n      <td>31</td>\n      <td>0.589156</td>\n      <td>0.594429</td>\n      <td>0.588857</td>\n      <td>0.584317</td>\n      <td>0.589696</td>\n      <td>0.589291</td>\n      <td>0.003210</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.207090</td>\n      <td>0.604433</td>\n      <td>0.088020</td>\n      <td>0.056639</td>\n      <td>10</td>\n      <td>5</td>\n      <td>{'max_depth': 10, 'n_estimators': 5}</td>\n      <td>0.698409</td>\n      <td>0.699448</td>\n      <td>0.699448</td>\n      <td>...</td>\n      <td>0.000630</td>\n      <td>29</td>\n      <td>0.582252</td>\n      <td>0.582587</td>\n      <td>0.582832</td>\n      <td>0.582777</td>\n      <td>0.582886</td>\n      <td>0.582667</td>\n      <td>0.000231</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2.732533</td>\n      <td>0.430209</td>\n      <td>0.095312</td>\n      <td>0.018309</td>\n      <td>10</td>\n      <td>10</td>\n      <td>{'max_depth': 10, 'n_estimators': 10}</td>\n      <td>0.700619</td>\n      <td>0.702983</td>\n      <td>0.702541</td>\n      <td>...</td>\n      <td>0.000799</td>\n      <td>25</td>\n      <td>0.580532</td>\n      <td>0.581419</td>\n      <td>0.582009</td>\n      <td>0.581864</td>\n      <td>0.581501</td>\n      <td>0.581465</td>\n      <td>0.000516</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6.513281</td>\n      <td>0.355321</td>\n      <td>0.117932</td>\n      <td>0.005480</td>\n      <td>10</td>\n      <td>50</td>\n      <td>{'max_depth': 10, 'n_estimators': 50}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12.563625</td>\n      <td>0.611457</td>\n      <td>0.186201</td>\n      <td>0.006912</td>\n      <td>10</td>\n      <td>100</td>\n      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n      <td>0.702607</td>\n      <td>0.702983</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000153</td>\n      <td>11</td>\n      <td>0.579883</td>\n      <td>0.580604</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.580092</td>\n      <td>0.000280</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>23.777351</td>\n      <td>1.339645</td>\n      <td>0.309892</td>\n      <td>0.012089</td>\n      <td>10</td>\n      <td>200</td>\n      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>35.656549</td>\n      <td>1.933852</td>\n      <td>0.442049</td>\n      <td>0.015196</td>\n      <td>10</td>\n      <td>300</td>\n      <td>{'max_depth': 10, 'n_estimators': 300}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.297402</td>\n      <td>0.064220</td>\n      <td>0.067397</td>\n      <td>0.007868</td>\n      <td>15</td>\n      <td>3</td>\n      <td>{'max_depth': 15, 'n_estimators': 3}</td>\n      <td>0.667035</td>\n      <td>0.673370</td>\n      <td>0.675138</td>\n      <td>...</td>\n      <td>0.002985</td>\n      <td>35</td>\n      <td>0.600619</td>\n      <td>0.591969</td>\n      <td>0.601001</td>\n      <td>0.594990</td>\n      <td>0.604648</td>\n      <td>0.598646</td>\n      <td>0.004548</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.652322</td>\n      <td>0.086871</td>\n      <td>0.073771</td>\n      <td>0.011680</td>\n      <td>15</td>\n      <td>5</td>\n      <td>{'max_depth': 15, 'n_estimators': 5}</td>\n      <td>0.690897</td>\n      <td>0.690166</td>\n      <td>0.687072</td>\n      <td>...</td>\n      <td>0.001727</td>\n      <td>32</td>\n      <td>0.598027</td>\n      <td>0.593211</td>\n      <td>0.595335</td>\n      <td>0.595755</td>\n      <td>0.600106</td>\n      <td>0.596487</td>\n      <td>0.002369</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2.617353</td>\n      <td>0.096881</td>\n      <td>0.084311</td>\n      <td>0.010682</td>\n      <td>15</td>\n      <td>10</td>\n      <td>{'max_depth': 15, 'n_estimators': 10}</td>\n      <td>0.694874</td>\n      <td>0.700110</td>\n      <td>0.701657</td>\n      <td>...</td>\n      <td>0.002279</td>\n      <td>30</td>\n      <td>0.587992</td>\n      <td>0.586681</td>\n      <td>0.592849</td>\n      <td>0.587665</td>\n      <td>0.586961</td>\n      <td>0.588430</td>\n      <td>0.002259</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>9.065110</td>\n      <td>0.445583</td>\n      <td>0.159000</td>\n      <td>0.018188</td>\n      <td>15</td>\n      <td>50</td>\n      <td>{'max_depth': 15, 'n_estimators': 50}</td>\n      <td>0.703270</td>\n      <td>0.702762</td>\n      <td>0.702983</td>\n      <td>...</td>\n      <td>0.000213</td>\n      <td>5</td>\n      <td>0.581837</td>\n      <td>0.580087</td>\n      <td>0.580604</td>\n      <td>0.581527</td>\n      <td>0.580722</td>\n      <td>0.580955</td>\n      <td>0.000638</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>17.957699</td>\n      <td>0.952308</td>\n      <td>0.264194</td>\n      <td>0.028548</td>\n      <td>15</td>\n      <td>100</td>\n      <td>{'max_depth': 15, 'n_estimators': 100}</td>\n      <td>0.702386</td>\n      <td>0.702762</td>\n      <td>0.702983</td>\n      <td>...</td>\n      <td>0.000251</td>\n      <td>22</td>\n      <td>0.579776</td>\n      <td>0.580087</td>\n      <td>0.580604</td>\n      <td>0.580087</td>\n      <td>0.579690</td>\n      <td>0.580049</td>\n      <td>0.000321</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>34.530249</td>\n      <td>0.917709</td>\n      <td>0.473735</td>\n      <td>0.022461</td>\n      <td>15</td>\n      <td>200</td>\n      <td>{'max_depth': 15, 'n_estimators': 200}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>13</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>51.818509</td>\n      <td>1.984330</td>\n      <td>0.723452</td>\n      <td>0.152528</td>\n      <td>15</td>\n      <td>300</td>\n      <td>{'max_depth': 15, 'n_estimators': 300}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702983</td>\n      <td>...</td>\n      <td>0.000153</td>\n      <td>11</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580604</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.580092</td>\n      <td>0.000280</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1.516421</td>\n      <td>0.084736</td>\n      <td>0.069156</td>\n      <td>0.006050</td>\n      <td>20</td>\n      <td>3</td>\n      <td>{'max_depth': 20, 'n_estimators': 3}</td>\n      <td>0.646929</td>\n      <td>0.650387</td>\n      <td>0.636243</td>\n      <td>...</td>\n      <td>0.009766</td>\n      <td>38</td>\n      <td>0.600946</td>\n      <td>0.601770</td>\n      <td>0.598442</td>\n      <td>0.594766</td>\n      <td>0.605648</td>\n      <td>0.600314</td>\n      <td>0.003613</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2.016819</td>\n      <td>0.109660</td>\n      <td>0.079316</td>\n      <td>0.015404</td>\n      <td>20</td>\n      <td>5</td>\n      <td>{'max_depth': 20, 'n_estimators': 5}</td>\n      <td>0.665488</td>\n      <td>0.662762</td>\n      <td>0.662983</td>\n      <td>...</td>\n      <td>0.001549</td>\n      <td>36</td>\n      <td>0.597100</td>\n      <td>0.595723</td>\n      <td>0.604110</td>\n      <td>0.604445</td>\n      <td>0.594261</td>\n      <td>0.599128</td>\n      <td>0.004301</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>3.094194</td>\n      <td>0.086147</td>\n      <td>0.081997</td>\n      <td>0.002285</td>\n      <td>20</td>\n      <td>10</td>\n      <td>{'max_depth': 20, 'n_estimators': 10}</td>\n      <td>0.684490</td>\n      <td>0.687072</td>\n      <td>0.689724</td>\n      <td>...</td>\n      <td>0.002226</td>\n      <td>33</td>\n      <td>0.599097</td>\n      <td>0.591657</td>\n      <td>0.604104</td>\n      <td>0.595233</td>\n      <td>0.593513</td>\n      <td>0.596721</td>\n      <td>0.004434</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>11.446477</td>\n      <td>0.286786</td>\n      <td>0.192669</td>\n      <td>0.032974</td>\n      <td>20</td>\n      <td>50</td>\n      <td>{'max_depth': 20, 'n_estimators': 50}</td>\n      <td>0.701502</td>\n      <td>0.701878</td>\n      <td>0.703646</td>\n      <td>...</td>\n      <td>0.001126</td>\n      <td>26</td>\n      <td>0.581369</td>\n      <td>0.583023</td>\n      <td>0.584557</td>\n      <td>0.583319</td>\n      <td>0.579608</td>\n      <td>0.582375</td>\n      <td>0.001717</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>22.080496</td>\n      <td>1.056220</td>\n      <td>0.300064</td>\n      <td>0.037323</td>\n      <td>20</td>\n      <td>100</td>\n      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n      <td>0.701502</td>\n      <td>0.701657</td>\n      <td>0.702983</td>\n      <td>...</td>\n      <td>0.000627</td>\n      <td>24</td>\n      <td>0.579905</td>\n      <td>0.579958</td>\n      <td>0.581824</td>\n      <td>0.580496</td>\n      <td>0.579776</td>\n      <td>0.580392</td>\n      <td>0.000757</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>42.728838</td>\n      <td>2.159839</td>\n      <td>0.535035</td>\n      <td>0.031863</td>\n      <td>20</td>\n      <td>200</td>\n      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n      <td>0.702607</td>\n      <td>0.702320</td>\n      <td>0.704088</td>\n      <td>...</td>\n      <td>0.000657</td>\n      <td>9</td>\n      <td>0.580293</td>\n      <td>0.579873</td>\n      <td>0.583177</td>\n      <td>0.580087</td>\n      <td>0.579690</td>\n      <td>0.580624</td>\n      <td>0.001293</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>65.911409</td>\n      <td>3.546866</td>\n      <td>0.841862</td>\n      <td>0.076846</td>\n      <td>20</td>\n      <td>300</td>\n      <td>{'max_depth': 20, 'n_estimators': 300}</td>\n      <td>0.702607</td>\n      <td>0.702983</td>\n      <td>0.703425</td>\n      <td>...</td>\n      <td>0.000376</td>\n      <td>8</td>\n      <td>0.580293</td>\n      <td>0.580604</td>\n      <td>0.582041</td>\n      <td>0.580604</td>\n      <td>0.579690</td>\n      <td>0.580646</td>\n      <td>0.000773</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.617096</td>\n      <td>0.100042</td>\n      <td>0.068514</td>\n      <td>0.007319</td>\n      <td>25</td>\n      <td>3</td>\n      <td>{'max_depth': 25, 'n_estimators': 3}</td>\n      <td>0.620415</td>\n      <td>0.622541</td>\n      <td>0.626077</td>\n      <td>...</td>\n      <td>0.003283</td>\n      <td>41</td>\n      <td>0.597626</td>\n      <td>0.596128</td>\n      <td>0.598125</td>\n      <td>0.600043</td>\n      <td>0.591843</td>\n      <td>0.596753</td>\n      <td>0.002756</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2.360732</td>\n      <td>0.312713</td>\n      <td>0.103816</td>\n      <td>0.012775</td>\n      <td>25</td>\n      <td>5</td>\n      <td>{'max_depth': 25, 'n_estimators': 5}</td>\n      <td>0.640521</td>\n      <td>0.645083</td>\n      <td>0.640000</td>\n      <td>...</td>\n      <td>0.003876</td>\n      <td>39</td>\n      <td>0.588993</td>\n      <td>0.593984</td>\n      <td>0.596505</td>\n      <td>0.593245</td>\n      <td>0.599339</td>\n      <td>0.594413</td>\n      <td>0.003452</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>3.223116</td>\n      <td>0.144879</td>\n      <td>0.091914</td>\n      <td>0.009295</td>\n      <td>25</td>\n      <td>10</td>\n      <td>{'max_depth': 25, 'n_estimators': 10}</td>\n      <td>0.666372</td>\n      <td>0.676685</td>\n      <td>0.680442</td>\n      <td>...</td>\n      <td>0.005074</td>\n      <td>34</td>\n      <td>0.595260</td>\n      <td>0.606527</td>\n      <td>0.599791</td>\n      <td>0.592553</td>\n      <td>0.598654</td>\n      <td>0.598557</td>\n      <td>0.004732</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>12.785458</td>\n      <td>0.816343</td>\n      <td>0.196002</td>\n      <td>0.019606</td>\n      <td>25</td>\n      <td>50</td>\n      <td>{'max_depth': 25, 'n_estimators': 50}</td>\n      <td>0.700619</td>\n      <td>0.702099</td>\n      <td>0.702983</td>\n      <td>...</td>\n      <td>0.001083</td>\n      <td>27</td>\n      <td>0.583830</td>\n      <td>0.586327</td>\n      <td>0.586253</td>\n      <td>0.584284</td>\n      <td>0.582208</td>\n      <td>0.584580</td>\n      <td>0.001557</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>24.348558</td>\n      <td>1.302333</td>\n      <td>0.313023</td>\n      <td>0.015366</td>\n      <td>25</td>\n      <td>100</td>\n      <td>{'max_depth': 25, 'n_estimators': 100}</td>\n      <td>0.701723</td>\n      <td>0.702099</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000675</td>\n      <td>21</td>\n      <td>0.580418</td>\n      <td>0.581389</td>\n      <td>0.582118</td>\n      <td>0.582958</td>\n      <td>0.582851</td>\n      <td>0.581947</td>\n      <td>0.000951</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>48.907190</td>\n      <td>3.604454</td>\n      <td>0.576040</td>\n      <td>0.017595</td>\n      <td>25</td>\n      <td>200</td>\n      <td>{'max_depth': 25, 'n_estimators': 200}</td>\n      <td>0.702386</td>\n      <td>0.702983</td>\n      <td>0.704309</td>\n      <td>...</td>\n      <td>0.000679</td>\n      <td>3</td>\n      <td>0.580185</td>\n      <td>0.581824</td>\n      <td>0.584890</td>\n      <td>0.581120</td>\n      <td>0.580614</td>\n      <td>0.581727</td>\n      <td>0.001673</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>70.561986</td>\n      <td>3.151758</td>\n      <td>0.750519</td>\n      <td>0.042283</td>\n      <td>25</td>\n      <td>300</td>\n      <td>{'max_depth': 25, 'n_estimators': 300}</td>\n      <td>0.703049</td>\n      <td>0.703425</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000296</td>\n      <td>7</td>\n      <td>0.580991</td>\n      <td>0.582041</td>\n      <td>0.581310</td>\n      <td>0.579980</td>\n      <td>0.580830</td>\n      <td>0.581030</td>\n      <td>0.000670</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1.735697</td>\n      <td>0.103134</td>\n      <td>0.068800</td>\n      <td>0.005705</td>\n      <td>30</td>\n      <td>3</td>\n      <td>{'max_depth': 30, 'n_estimators': 3}</td>\n      <td>0.621078</td>\n      <td>0.615028</td>\n      <td>0.619448</td>\n      <td>...</td>\n      <td>0.008529</td>\n      <td>42</td>\n      <td>0.598605</td>\n      <td>0.592918</td>\n      <td>0.592053</td>\n      <td>0.582059</td>\n      <td>0.588421</td>\n      <td>0.590811</td>\n      <td>0.005460</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>2.123360</td>\n      <td>0.090228</td>\n      <td>0.078598</td>\n      <td>0.010842</td>\n      <td>30</td>\n      <td>5</td>\n      <td>{'max_depth': 30, 'n_estimators': 5}</td>\n      <td>0.629253</td>\n      <td>0.629171</td>\n      <td>0.641326</td>\n      <td>...</td>\n      <td>0.007835</td>\n      <td>40</td>\n      <td>0.595096</td>\n      <td>0.593417</td>\n      <td>0.600902</td>\n      <td>0.597264</td>\n      <td>0.610628</td>\n      <td>0.599462</td>\n      <td>0.006119</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>3.396755</td>\n      <td>0.233423</td>\n      <td>0.094752</td>\n      <td>0.018988</td>\n      <td>30</td>\n      <td>10</td>\n      <td>{'max_depth': 30, 'n_estimators': 10}</td>\n      <td>0.662837</td>\n      <td>0.655028</td>\n      <td>0.665856</td>\n      <td>...</td>\n      <td>0.004628</td>\n      <td>37</td>\n      <td>0.607125</td>\n      <td>0.590571</td>\n      <td>0.607030</td>\n      <td>0.601675</td>\n      <td>0.604066</td>\n      <td>0.602094</td>\n      <td>0.006108</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>14.091371</td>\n      <td>1.477842</td>\n      <td>0.195708</td>\n      <td>0.008198</td>\n      <td>30</td>\n      <td>50</td>\n      <td>{'max_depth': 30, 'n_estimators': 50}</td>\n      <td>0.699072</td>\n      <td>0.701878</td>\n      <td>0.704088</td>\n      <td>...</td>\n      <td>0.001651</td>\n      <td>28</td>\n      <td>0.585024</td>\n      <td>0.589106</td>\n      <td>0.593243</td>\n      <td>0.586338</td>\n      <td>0.587320</td>\n      <td>0.588206</td>\n      <td>0.002849</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>25.329111</td>\n      <td>1.105834</td>\n      <td>0.333201</td>\n      <td>0.006652</td>\n      <td>30</td>\n      <td>100</td>\n      <td>{'max_depth': 30, 'n_estimators': 100}</td>\n      <td>0.701502</td>\n      <td>0.702762</td>\n      <td>0.703867</td>\n      <td>...</td>\n      <td>0.000877</td>\n      <td>6</td>\n      <td>0.581442</td>\n      <td>0.584507</td>\n      <td>0.588183</td>\n      <td>0.583319</td>\n      <td>0.585675</td>\n      <td>0.584625</td>\n      <td>0.002263</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>45.932872</td>\n      <td>0.813640</td>\n      <td>0.435839</td>\n      <td>0.028102</td>\n      <td>30</td>\n      <td>200</td>\n      <td>{'max_depth': 30, 'n_estimators': 200}</td>\n      <td>0.703049</td>\n      <td>0.703646</td>\n      <td>0.703867</td>\n      <td>...</td>\n      <td>0.000347</td>\n      <td>1</td>\n      <td>0.581729</td>\n      <td>0.582555</td>\n      <td>0.584270</td>\n      <td>0.581527</td>\n      <td>0.581643</td>\n      <td>0.582345</td>\n      <td>0.001029</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>55.486510</td>\n      <td>2.852526</td>\n      <td>0.533100</td>\n      <td>0.042226</td>\n      <td>30</td>\n      <td>300</td>\n      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n      <td>0.703491</td>\n      <td>0.703204</td>\n      <td>0.703646</td>\n      <td>...</td>\n      <td>0.000466</td>\n      <td>4</td>\n      <td>0.581946</td>\n      <td>0.582337</td>\n      <td>0.583360</td>\n      <td>0.580389</td>\n      <td>0.580207</td>\n      <td>0.581648</td>\n      <td>0.001197</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n<p>42 rows  39 columns</p>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [3, 5, 10, 50, 100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30]\n",
    "}\n",
    "rf = RandomForestClassifier(max_features='auto')\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], refit=False, n_jobs=-1)\n",
    "grid_search.fit(X_train_ohe_mi, y_train)\n",
    "search_results_rf = pd.DataFrame(grid_search.cv_results_)\n",
    "search_results_rf.to_csv(r\"rf_grid_search_results.csv\", index=False)\n",
    "search_results_rf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:54:01.437646400Z",
     "start_time": "2023-05-16T15:49:49.720020100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0     756.600943     25.065281       109.739918        4.363684     0.1   \n1    1097.978713     68.093132       125.171618        2.320846       1   \n2    1236.406781     80.424644       127.721138        1.305291       2   \n3    1126.945769     63.613184       105.985401       11.436425     2.5   \n4    1011.467553     66.197054        66.977808       13.584966       3   \n5     925.348961     61.167750        43.663795        8.280008       5   \n\n       params  split0_test_accuracy  split1_test_accuracy  \\\n0  {'C': 0.1}              0.702607              0.702762   \n1    {'C': 1}              0.702607              0.702762   \n2    {'C': 2}              0.702607              0.702762   \n3  {'C': 2.5}              0.702607              0.702762   \n4    {'C': 3}              0.702607              0.702762   \n5    {'C': 5}              0.702165              0.702762   \n\n   split2_test_accuracy  split3_test_accuracy  ...  std_test_recall_weighted  \\\n0              0.702762              0.702762  ...                  0.000094   \n1              0.702762              0.702762  ...                  0.000094   \n2              0.702762              0.702541  ...                  0.000100   \n3              0.702762              0.702541  ...                  0.000164   \n4              0.702762              0.702541  ...                  0.000244   \n5              0.702541              0.701878  ...                  0.000319   \n\n   rank_test_recall_weighted  split0_test_f1_weighted  \\\n0                          1                 0.579883   \n1                          1                 0.579883   \n2                          3                 0.579883   \n3                          4                 0.579883   \n4                          5                 0.579883   \n5                          6                 0.579669   \n\n   split1_test_f1_weighted  split2_test_f1_weighted  split3_test_f1_weighted  \\\n0                 0.580087                 0.580087                 0.580087   \n1                 0.580087                 0.580087                 0.580087   \n2                 0.580087                 0.580087                 0.579980   \n3                 0.580087                 0.580087                 0.579980   \n4                 0.580087                 0.580087                 0.579980   \n5                 0.580087                 0.580055                 0.579658   \n\n   split4_test_f1_weighted  mean_test_f1_weighted  std_test_f1_weighted  \\\n0                 0.579797               0.579988              0.000124   \n1                 0.579797               0.579988              0.000124   \n2                 0.579797               0.579967              0.000114   \n3                 0.579690               0.579945              0.000148   \n4                 0.579583               0.579924              0.000187   \n5                 0.579583               0.579810              0.000215   \n\n   rank_test_f1_weighted  \n0                      1  \n1                      1  \n2                      3  \n3                      4  \n4                      5  \n5                      6  \n\n[6 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>params</th>\n      <th>split0_test_accuracy</th>\n      <th>split1_test_accuracy</th>\n      <th>split2_test_accuracy</th>\n      <th>split3_test_accuracy</th>\n      <th>...</th>\n      <th>std_test_recall_weighted</th>\n      <th>rank_test_recall_weighted</th>\n      <th>split0_test_f1_weighted</th>\n      <th>split1_test_f1_weighted</th>\n      <th>split2_test_f1_weighted</th>\n      <th>split3_test_f1_weighted</th>\n      <th>split4_test_f1_weighted</th>\n      <th>mean_test_f1_weighted</th>\n      <th>std_test_f1_weighted</th>\n      <th>rank_test_f1_weighted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>756.600943</td>\n      <td>25.065281</td>\n      <td>109.739918</td>\n      <td>4.363684</td>\n      <td>0.1</td>\n      <td>{'C': 0.1}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>1</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1097.978713</td>\n      <td>68.093132</td>\n      <td>125.171618</td>\n      <td>2.320846</td>\n      <td>1</td>\n      <td>{'C': 1}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>...</td>\n      <td>0.000094</td>\n      <td>1</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579797</td>\n      <td>0.579988</td>\n      <td>0.000124</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1236.406781</td>\n      <td>80.424644</td>\n      <td>127.721138</td>\n      <td>1.305291</td>\n      <td>2</td>\n      <td>{'C': 2}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>0.702541</td>\n      <td>...</td>\n      <td>0.000100</td>\n      <td>3</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579980</td>\n      <td>0.579797</td>\n      <td>0.579967</td>\n      <td>0.000114</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1126.945769</td>\n      <td>63.613184</td>\n      <td>105.985401</td>\n      <td>11.436425</td>\n      <td>2.5</td>\n      <td>{'C': 2.5}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>0.702541</td>\n      <td>...</td>\n      <td>0.000164</td>\n      <td>4</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579980</td>\n      <td>0.579690</td>\n      <td>0.579945</td>\n      <td>0.000148</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1011.467553</td>\n      <td>66.197054</td>\n      <td>66.977808</td>\n      <td>13.584966</td>\n      <td>3</td>\n      <td>{'C': 3}</td>\n      <td>0.702607</td>\n      <td>0.702762</td>\n      <td>0.702762</td>\n      <td>0.702541</td>\n      <td>...</td>\n      <td>0.000244</td>\n      <td>5</td>\n      <td>0.579883</td>\n      <td>0.580087</td>\n      <td>0.580087</td>\n      <td>0.579980</td>\n      <td>0.579583</td>\n      <td>0.579924</td>\n      <td>0.000187</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>925.348961</td>\n      <td>61.167750</td>\n      <td>43.663795</td>\n      <td>8.280008</td>\n      <td>5</td>\n      <td>{'C': 5}</td>\n      <td>0.702165</td>\n      <td>0.702762</td>\n      <td>0.702541</td>\n      <td>0.701878</td>\n      <td>...</td>\n      <td>0.000319</td>\n      <td>6</td>\n      <td>0.579669</td>\n      <td>0.580087</td>\n      <td>0.580055</td>\n      <td>0.579658</td>\n      <td>0.579583</td>\n      <td>0.579810</td>\n      <td>0.000215</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows  38 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 2, 2.5, 3, 5],\n",
    "}\n",
    "svc = SVC(kernel='rbf', gamma='auto')\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], refit=False, n_jobs=-1)\n",
    "grid_search.fit(X_train_ohe_mi, y_train)\n",
    "search_results_svc = pd.DataFrame(grid_search.cv_results_)\n",
    "search_results_svc.to_csv(r\"svc_grid_search_results.csv\", index=False)\n",
    "search_results_svc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T16:46:03.005682300Z",
     "start_time": "2023-05-16T16:07:34.977693500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 2, 2.5, 3, 5],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "svc = SVC(kernel='poly', gamma='auto')\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], refit=False, n_jobs=-1)\n",
    "grid_search.fit(X_train_ohe_mi, y_train)\n",
    "search_results_svc_poly = pd.DataFrame(grid_search.cv_results_)\n",
    "search_results_svc_poly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n0     101.656235      6.232892         0.083405        0.007197     0.1   \n1      97.870135      5.581130         0.085600        0.008185       1   \n2     104.876216      4.464502         0.101314        0.013848       2   \n3      94.806154      8.364259         0.076405        0.006887     2.5   \n4      89.741294      1.542235         0.048556        0.008341       3   \n5      83.931258      1.236362         0.035839        0.004459       5   \n\n       params  split0_test_accuracy  split1_test_accuracy  \\\n0  {'C': 0.1}              0.704817              0.488177   \n1    {'C': 1}              0.703049              0.498122   \n2    {'C': 2}              0.296288              0.407072   \n3  {'C': 2.5}              0.702386              0.573039   \n4    {'C': 3}              0.701502              0.699448   \n5    {'C': 5}              0.238400              0.698564   \n\n   split2_test_accuracy  split3_test_accuracy  ...  std_test_recall_weighted  \\\n0              0.703204              0.666298  ...                  0.083670   \n1              0.697901              0.272265  ...                  0.161329   \n2              0.425414              0.685304  ...                  0.161323   \n3              0.373039              0.509613  ...                  0.116200   \n4              0.701878              0.673591  ...                  0.011000   \n5              0.290829              0.550055  ...                  0.179531   \n\n   rank_test_recall_weighted  split0_test_f1_weighted  \\\n0                          2                 0.589723   \n1                          4                 0.589963   \n2                          5                 0.201274   \n3                          3                 0.586555   \n4                          1                 0.588842   \n5                          6                 0.234016   \n\n   split1_test_f1_weighted  split2_test_f1_weighted  split3_test_f1_weighted  \\\n0                 0.524488                 0.581119                 0.599943   \n1                 0.532402                 0.592499                 0.148023   \n2                 0.463022                 0.433702                 0.600206   \n3                 0.569176                 0.354531                 0.540724   \n4                 0.588006                 0.581686                 0.603032   \n5                 0.587029                 0.187283                 0.558059   \n\n   split4_test_f1_weighted  mean_test_f1_weighted  std_test_f1_weighted  \\\n0                 0.581426               0.575340              0.026338   \n1                 0.482648               0.469107              0.165600   \n2                 0.585424               0.456726              0.143509   \n3                 0.597796               0.529756              0.089703   \n4                 0.592296               0.590772              0.007022   \n5                 0.189669               0.351211              0.181714   \n\n   rank_test_f1_weighted  \n0                      2  \n1                      4  \n2                      5  \n3                      3  \n4                      1  \n5                      6  \n\n[6 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_C</th>\n      <th>params</th>\n      <th>split0_test_accuracy</th>\n      <th>split1_test_accuracy</th>\n      <th>split2_test_accuracy</th>\n      <th>split3_test_accuracy</th>\n      <th>...</th>\n      <th>std_test_recall_weighted</th>\n      <th>rank_test_recall_weighted</th>\n      <th>split0_test_f1_weighted</th>\n      <th>split1_test_f1_weighted</th>\n      <th>split2_test_f1_weighted</th>\n      <th>split3_test_f1_weighted</th>\n      <th>split4_test_f1_weighted</th>\n      <th>mean_test_f1_weighted</th>\n      <th>std_test_f1_weighted</th>\n      <th>rank_test_f1_weighted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101.656235</td>\n      <td>6.232892</td>\n      <td>0.083405</td>\n      <td>0.007197</td>\n      <td>0.1</td>\n      <td>{'C': 0.1}</td>\n      <td>0.704817</td>\n      <td>0.488177</td>\n      <td>0.703204</td>\n      <td>0.666298</td>\n      <td>...</td>\n      <td>0.083670</td>\n      <td>2</td>\n      <td>0.589723</td>\n      <td>0.524488</td>\n      <td>0.581119</td>\n      <td>0.599943</td>\n      <td>0.581426</td>\n      <td>0.575340</td>\n      <td>0.026338</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>97.870135</td>\n      <td>5.581130</td>\n      <td>0.085600</td>\n      <td>0.008185</td>\n      <td>1</td>\n      <td>{'C': 1}</td>\n      <td>0.703049</td>\n      <td>0.498122</td>\n      <td>0.697901</td>\n      <td>0.272265</td>\n      <td>...</td>\n      <td>0.161329</td>\n      <td>4</td>\n      <td>0.589963</td>\n      <td>0.532402</td>\n      <td>0.592499</td>\n      <td>0.148023</td>\n      <td>0.482648</td>\n      <td>0.469107</td>\n      <td>0.165600</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104.876216</td>\n      <td>4.464502</td>\n      <td>0.101314</td>\n      <td>0.013848</td>\n      <td>2</td>\n      <td>{'C': 2}</td>\n      <td>0.296288</td>\n      <td>0.407072</td>\n      <td>0.425414</td>\n      <td>0.685304</td>\n      <td>...</td>\n      <td>0.161323</td>\n      <td>5</td>\n      <td>0.201274</td>\n      <td>0.463022</td>\n      <td>0.433702</td>\n      <td>0.600206</td>\n      <td>0.585424</td>\n      <td>0.456726</td>\n      <td>0.143509</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>94.806154</td>\n      <td>8.364259</td>\n      <td>0.076405</td>\n      <td>0.006887</td>\n      <td>2.5</td>\n      <td>{'C': 2.5}</td>\n      <td>0.702386</td>\n      <td>0.573039</td>\n      <td>0.373039</td>\n      <td>0.509613</td>\n      <td>...</td>\n      <td>0.116200</td>\n      <td>3</td>\n      <td>0.586555</td>\n      <td>0.569176</td>\n      <td>0.354531</td>\n      <td>0.540724</td>\n      <td>0.597796</td>\n      <td>0.529756</td>\n      <td>0.089703</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>89.741294</td>\n      <td>1.542235</td>\n      <td>0.048556</td>\n      <td>0.008341</td>\n      <td>3</td>\n      <td>{'C': 3}</td>\n      <td>0.701502</td>\n      <td>0.699448</td>\n      <td>0.701878</td>\n      <td>0.673591</td>\n      <td>...</td>\n      <td>0.011000</td>\n      <td>1</td>\n      <td>0.588842</td>\n      <td>0.588006</td>\n      <td>0.581686</td>\n      <td>0.603032</td>\n      <td>0.592296</td>\n      <td>0.590772</td>\n      <td>0.007022</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>83.931258</td>\n      <td>1.236362</td>\n      <td>0.035839</td>\n      <td>0.004459</td>\n      <td>5</td>\n      <td>{'C': 5}</td>\n      <td>0.238400</td>\n      <td>0.698564</td>\n      <td>0.290829</td>\n      <td>0.550055</td>\n      <td>...</td>\n      <td>0.179531</td>\n      <td>6</td>\n      <td>0.234016</td>\n      <td>0.587029</td>\n      <td>0.187283</td>\n      <td>0.558059</td>\n      <td>0.189669</td>\n      <td>0.351211</td>\n      <td>0.181714</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows  38 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 2, 2.5, 3, 5],\n",
    "}\n",
    "svc = LinearSVC()\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'], refit=False, n_jobs=-1)\n",
    "grid_search.fit(X_train_ohe_mi, y_train)\n",
    "search_results_svc_linear = pd.DataFrame(grid_search.cv_results_)\n",
    "search_results_svc_linear.to_csv(r\"svc_linear_grid_search_results.csv\", index=False)\n",
    "search_results_svc_linear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T16:07:34.975688400Z",
     "start_time": "2023-05-16T16:04:11.701890600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'fit_time': array([28.15581942, 28.33726287, 28.13020492, 28.14064503, 28.02849722]),\n 'score_time': array([0.27001047, 0.25025582, 0.25397801, 0.26599216, 0.2770052 ]),\n 'test_accuracy': array([0.70304905, 0.70320442, 0.70276243, 0.70320442, 0.7040884 ]),\n 'train_accuracy': array([0.86679558, 0.84216342, 0.86636097, 0.8498978 , 0.85083697]),\n 'test_precision_weighted': array([0.68632654, 0.68658749, 0.62222534, 0.750668  , 0.70456142]),\n 'train_precision_weighted': array([0.88802201, 0.87111431, 0.88771603, 0.87631835, 0.87695501]),\n 'test_recall_weighted': array([0.70304905, 0.70320442, 0.70276243, 0.70320442, 0.7040884 ]),\n 'train_recall_weighted': array([0.86679558, 0.84216342, 0.86636097, 0.8498978 , 0.85083697]),\n 'test_f1_weighted': array([0.58132324, 0.58152704, 0.58049621, 0.58111991, 0.58420165]),\n 'train_f1_weighted': array([0.85214856, 0.82065416, 0.85156519, 0.83062223, 0.83149964])}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print prediction results of y_pred in each fold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto', n_estimators=200, max_depth=25)\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "cv_results = cross_validate(rf, X_train_ohe_mi, y_train, scoring=scoring, cv=5, return_train_score=True)\n",
    "y_pred = cross_val_predict(rf, X_train_ohe_mi, y_train, cv=5)\n",
    "cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T00:55:36.962613Z",
     "end_time": "2023-05-11T01:00:26.443215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3., 4.])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-11T01:01:09.183067Z",
     "end_time": "2023-05-11T01:01:09.239071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('project_data_files/book_rating_test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:39.115604200Z",
     "start_time": "2023-05-17T06:27:38.966555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# process test set\n",
    "test_df = test_df.drop(columns=['Language', 'PublishDay', 'Name', 'Authors', 'Description', 'PublishYear', 'PublishMonth'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:39.919866400Z",
     "start_time": "2023-05-17T06:27:39.836575700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# set outliers in pages number to nan\n",
    "mean = test_df['pagesNumber'].mean()\n",
    "sd = test_df['pagesNumber'].std()\n",
    "test_df['pagesNumber'] = test_df['pagesNumber'].apply(lambda x: np.nan if x > mean + 3*sd or x < mean - 3*sd else x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:41.290711Z",
     "start_time": "2023-05-17T06:27:41.219924200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "               Publisher  pagesNumber\n0            Orbis Books        118.0\n1                 Puffin         32.0\n2      Benjamin Cummings        544.0\n3                 Signet        432.0\n4     Thomas Dunne Books        352.0\n...                  ...          ...\n5761     Ten Speed Press        274.0\n5762       Ellora's Cave        224.0\n5763     Editorial Diana        224.0\n5764      Westview Press        242.0\n5765        Conari Press        192.0\n\n[5766 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher</th>\n      <th>pagesNumber</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Orbis Books</td>\n      <td>118.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Puffin</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Benjamin Cummings</td>\n      <td>544.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Signet</td>\n      <td>432.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Thomas Dunne Books</td>\n      <td>352.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5761</th>\n      <td>Ten Speed Press</td>\n      <td>274.0</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>Ellora's Cave</td>\n      <td>224.0</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>Editorial Diana</td>\n      <td>224.0</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>Westview Press</td>\n      <td>242.0</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>Conari Press</td>\n      <td>192.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  2 columns</p>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill nan values in pages number with mean\n",
    "test_df['pagesNumber'] = test_df['pagesNumber'].fillna(test_df['pagesNumber'].mean())\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:42.194494900Z",
     "start_time": "2023-05-17T06:27:42.106726100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "test_df['Publisher'] = test_df['Publisher'].fillna('Unknown')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:48.871988300Z",
     "start_time": "2023-05-17T06:27:48.804871200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "test_data_df = test_df['Publisher'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:51.713554100Z",
     "start_time": "2023-05-17T06:27:51.639724900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# mark publishers with less than 10 books as 'Other'\n",
    "test_df['Publisher'] = test_df['Publisher'].apply(lambda x: 'Other' if x in test_data_df[test_data_df < 10].index else x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:54.658225900Z",
     "start_time": "2023-05-17T06:27:53.515510500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "      Publisher_1st World Library - Literary Society  Publisher_ADV Manga  \\\n0                                                0.0                  0.0   \n1                                                0.0                  0.0   \n2                                                0.0                  0.0   \n3                                                0.0                  0.0   \n4                                                0.0                  0.0   \n...                                              ...                  ...   \n5761                                             0.0                  0.0   \n5762                                             0.0                  0.0   \n5763                                             0.0                  0.0   \n5764                                             0.0                  0.0   \n5765                                             0.0                  0.0   \n\n      Publisher_AMACOM/American Management Association  \\\n0                                                  0.0   \n1                                                  0.0   \n2                                                  0.0   \n3                                                  0.0   \n4                                                  0.0   \n...                                                ...   \n5761                                               0.0   \n5762                                               0.0   \n5763                                               0.0   \n5764                                               0.0   \n5765                                               0.0   \n\n      Publisher_Abbeville Press  Publisher_Abingdon Press  Publisher_Ace  \\\n0                           0.0                       0.0            0.0   \n1                           0.0                       0.0            0.0   \n2                           0.0                       0.0            0.0   \n3                           0.0                       0.0            0.0   \n4                           0.0                       0.0            0.0   \n...                         ...                       ...            ...   \n5761                        0.0                       0.0            0.0   \n5762                        0.0                       0.0            0.0   \n5763                        0.0                       0.0            0.0   \n5764                        0.0                       0.0            0.0   \n5765                        0.0                       0.0            0.0   \n\n      Publisher_Ace Books  Publisher_Adams Media  \\\n0                     0.0                    0.0   \n1                     0.0                    0.0   \n2                     0.0                    0.0   \n3                     0.0                    0.0   \n4                     0.0                    0.0   \n...                   ...                    ...   \n5761                  0.0                    0.0   \n5762                  0.0                    0.0   \n5763                  0.0                    0.0   \n5764                  0.0                    0.0   \n5765                  0.0                    0.0   \n\n      Publisher_Addison Wesley Publishing Company  \\\n0                                             0.0   \n1                                             0.0   \n2                                             0.0   \n3                                             0.0   \n4                                             0.0   \n...                                           ...   \n5761                                          0.0   \n5762                                          0.0   \n5763                                          0.0   \n5764                                          0.0   \n5765                                          0.0   \n\n      Publisher_Addison-Wesley Professional  ...  Publisher_Yearling  \\\n0                                       0.0  ...                 0.0   \n1                                       0.0  ...                 0.0   \n2                                       0.0  ...                 0.0   \n3                                       0.0  ...                 0.0   \n4                                       0.0  ...                 0.0   \n...                                     ...  ...                 ...   \n5761                                    0.0  ...                 0.0   \n5762                                    0.0  ...                 0.0   \n5763                                    0.0  ...                 0.0   \n5764                                    0.0  ...                 0.0   \n5765                                    0.0  ...                 0.0   \n\n      Publisher_Zebra  Publisher_Zed Books  Publisher_Zonderkidz  \\\n0                 0.0                  0.0                   0.0   \n1                 0.0                  0.0                   0.0   \n2                 0.0                  0.0                   0.0   \n3                 0.0                  0.0                   0.0   \n4                 0.0                  0.0                   0.0   \n...               ...                  ...                   ...   \n5761              0.0                  0.0                   0.0   \n5762              0.0                  0.0                   0.0   \n5763              0.0                  0.0                   0.0   \n5764              0.0                  0.0                   0.0   \n5765              0.0                  0.0                   0.0   \n\n      Publisher_Zondervan  Publisher_Zondervan Academic  \\\n0                     0.0                           0.0   \n1                     0.0                           0.0   \n2                     0.0                           0.0   \n3                     0.0                           0.0   \n4                     0.0                           0.0   \n...                   ...                           ...   \n5761                  0.0                           0.0   \n5762                  0.0                           0.0   \n5763                  0.0                           0.0   \n5764                  0.0                           0.0   \n5765                  0.0                           0.0   \n\n      Publisher_Zondervan Publishing Company  Publisher_eReads.com  \\\n0                                        0.0                   0.0   \n1                                        0.0                   0.0   \n2                                        0.0                   0.0   \n3                                        0.0                   0.0   \n4                                        0.0                   0.0   \n...                                      ...                   ...   \n5761                                     0.0                   0.0   \n5762                                     0.0                   0.0   \n5763                                     0.0                   0.0   \n5764                                     0.0                   0.0   \n5765                                     0.0                   0.0   \n\n      Publisher_iBooks  Publisher_iUniverse  \n0                  0.0                  0.0  \n1                  0.0                  0.0  \n2                  0.0                  0.0  \n3                  0.0                  0.0  \n4                  0.0                  0.0  \n...                ...                  ...  \n5761               0.0                  0.0  \n5762               0.0                  0.0  \n5763               0.0                  0.0  \n5764               0.0                  0.0  \n5765               0.0                  0.0  \n\n[5766 rows x 509 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Publisher_1st World Library - Literary Society</th>\n      <th>Publisher_ADV Manga</th>\n      <th>Publisher_AMACOM/American Management Association</th>\n      <th>Publisher_Abbeville Press</th>\n      <th>Publisher_Abingdon Press</th>\n      <th>Publisher_Ace</th>\n      <th>Publisher_Ace Books</th>\n      <th>Publisher_Adams Media</th>\n      <th>Publisher_Addison Wesley Publishing Company</th>\n      <th>Publisher_Addison-Wesley Professional</th>\n      <th>...</th>\n      <th>Publisher_Yearling</th>\n      <th>Publisher_Zebra</th>\n      <th>Publisher_Zed Books</th>\n      <th>Publisher_Zonderkidz</th>\n      <th>Publisher_Zondervan</th>\n      <th>Publisher_Zondervan Academic</th>\n      <th>Publisher_Zondervan Publishing Company</th>\n      <th>Publisher_eReads.com</th>\n      <th>Publisher_iBooks</th>\n      <th>Publisher_iUniverse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5761</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  509 columns</p>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_ohe = enc.transform(test_df[['Publisher']])\n",
    "test_df_ohe = pd.DataFrame.sparse.from_spmatrix(test_df_ohe)\n",
    "test_df_ohe.columns = feature_names\n",
    "test_df_ohe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:56.758757900Z",
     "start_time": "2023-05-17T06:27:56.592029400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "      0    1    2    3    4    5    6    7    8    9    ...  440  441  442  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n5761  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n5762  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n5763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n5764  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n5765  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n\n      443  444  445  446  447  448  449  \n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n...   ...  ...  ...  ...  ...  ...  ...  \n5761  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n5762  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n5763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n5764  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n5765  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[5766 rows x 450 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>440</th>\n      <th>441</th>\n      <th>442</th>\n      <th>443</th>\n      <th>444</th>\n      <th>445</th>\n      <th>446</th>\n      <th>447</th>\n      <th>448</th>\n      <th>449</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5761</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  450 columns</p>\n</div>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_ohe_mi = mi.transform(test_df_ohe)\n",
    "test_df_ohe_mi = pd.DataFrame.sparse.from_spmatrix(test_df_ohe_mi)\n",
    "test_df_ohe_mi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:27:59.073563300Z",
     "start_time": "2023-05-17T06:27:58.973205500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "        0    1    2    3    4    5    6    7    8    9  ...  Author_10  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.329671   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.400349   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.225617   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.133304   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.224210   \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...        ...   \n5761  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.189251   \n5762  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.090290   \n5763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.119326   \n5764  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.430216   \n5765  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.176953   \n\n      Author_11  Author_12  Author_13  Author_14  Author_15  Author_16  \\\n0      0.343979   0.018261   0.115687  -0.111172   0.068306   0.158065   \n1      0.065201   0.349188   0.020555   0.281087   0.231422   0.129853   \n2     -0.004355   0.173353   0.087015   0.106534   0.040950   0.209152   \n3     -0.069995   0.206028   0.089625   0.157605   0.131767   0.244849   \n4      0.049880   0.003623   0.062291  -0.030742   0.130882   0.295086   \n...         ...        ...        ...        ...        ...        ...   \n5761  -0.075898   0.156563  -0.005267   0.044208   0.017580   0.004008   \n5762  -0.034920   0.157064   0.029945   0.082974  -0.053072  -0.040132   \n5763   0.029966  -0.105229  -0.029109   0.143553  -0.038797   0.285414   \n5764  -0.045813   0.296560   0.222057   0.132400   0.003003   0.249677   \n5765  -0.101169   0.179290   0.128011   0.113272   0.070050   0.206951   \n\n      Author_17  Author_18  Author_19  \n0      0.053510  -0.136804  -0.084448  \n1     -0.213233  -0.081253  -0.204687  \n2     -0.215313  -0.177547  -0.178094  \n3     -0.321698  -0.198365  -0.208098  \n4     -0.061550  -0.244197  -0.272161  \n...         ...        ...        ...  \n5761  -0.134761  -0.113087  -0.110519  \n5762  -0.116865  -0.030149  -0.065287  \n5763  -0.234488  -0.086112  -0.240196  \n5764  -0.271907  -0.262187  -0.313596  \n5765  -0.166522  -0.197051  -0.169054  \n\n[5766 rows x 470 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>Author_10</th>\n      <th>Author_11</th>\n      <th>Author_12</th>\n      <th>Author_13</th>\n      <th>Author_14</th>\n      <th>Author_15</th>\n      <th>Author_16</th>\n      <th>Author_17</th>\n      <th>Author_18</th>\n      <th>Author_19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.329671</td>\n      <td>0.343979</td>\n      <td>0.018261</td>\n      <td>0.115687</td>\n      <td>-0.111172</td>\n      <td>0.068306</td>\n      <td>0.158065</td>\n      <td>0.053510</td>\n      <td>-0.136804</td>\n      <td>-0.084448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.400349</td>\n      <td>0.065201</td>\n      <td>0.349188</td>\n      <td>0.020555</td>\n      <td>0.281087</td>\n      <td>0.231422</td>\n      <td>0.129853</td>\n      <td>-0.213233</td>\n      <td>-0.081253</td>\n      <td>-0.204687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.225617</td>\n      <td>-0.004355</td>\n      <td>0.173353</td>\n      <td>0.087015</td>\n      <td>0.106534</td>\n      <td>0.040950</td>\n      <td>0.209152</td>\n      <td>-0.215313</td>\n      <td>-0.177547</td>\n      <td>-0.178094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.133304</td>\n      <td>-0.069995</td>\n      <td>0.206028</td>\n      <td>0.089625</td>\n      <td>0.157605</td>\n      <td>0.131767</td>\n      <td>0.244849</td>\n      <td>-0.321698</td>\n      <td>-0.198365</td>\n      <td>-0.208098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.224210</td>\n      <td>0.049880</td>\n      <td>0.003623</td>\n      <td>0.062291</td>\n      <td>-0.030742</td>\n      <td>0.130882</td>\n      <td>0.295086</td>\n      <td>-0.061550</td>\n      <td>-0.244197</td>\n      <td>-0.272161</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5761</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.189251</td>\n      <td>-0.075898</td>\n      <td>0.156563</td>\n      <td>-0.005267</td>\n      <td>0.044208</td>\n      <td>0.017580</td>\n      <td>0.004008</td>\n      <td>-0.134761</td>\n      <td>-0.113087</td>\n      <td>-0.110519</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.090290</td>\n      <td>-0.034920</td>\n      <td>0.157064</td>\n      <td>0.029945</td>\n      <td>0.082974</td>\n      <td>-0.053072</td>\n      <td>-0.040132</td>\n      <td>-0.116865</td>\n      <td>-0.030149</td>\n      <td>-0.065287</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.119326</td>\n      <td>0.029966</td>\n      <td>-0.105229</td>\n      <td>-0.029109</td>\n      <td>0.143553</td>\n      <td>-0.038797</td>\n      <td>0.285414</td>\n      <td>-0.234488</td>\n      <td>-0.086112</td>\n      <td>-0.240196</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.430216</td>\n      <td>-0.045813</td>\n      <td>0.296560</td>\n      <td>0.222057</td>\n      <td>0.132400</td>\n      <td>0.003003</td>\n      <td>0.249677</td>\n      <td>-0.271907</td>\n      <td>-0.262187</td>\n      <td>-0.313596</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.176953</td>\n      <td>-0.101169</td>\n      <td>0.179290</td>\n      <td>0.128011</td>\n      <td>0.113272</td>\n      <td>0.070050</td>\n      <td>0.206951</td>\n      <td>-0.166522</td>\n      <td>-0.197051</td>\n      <td>-0.169054</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  470 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "for i in range(20):\n",
    "    test_df_ohe_mi['Author_' + str(i)] = author_vec[i]\n",
    "\n",
    "test_df_ohe_mi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:05.042152Z",
     "start_time": "2023-05-17T06:28:04.854654400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_35544\\2509528956.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n"
     ]
    },
    {
     "data": {
      "text/plain": "     0    1    2    3    4    5    6    7    8    9  ...   Name_90   Name_91  \\\n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.172811  0.098389   \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.245650 -0.049657   \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033781  0.093943   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.020762 -0.149720   \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.191644  0.044182   \n\n    Name_92   Name_93   Name_94   Name_95   Name_96   Name_97   Name_98  \\\n0 -0.062941  0.118057 -0.065377  0.227973  0.218879 -0.151266 -0.048105   \n1  0.072740 -0.055925 -0.000046  0.140500  0.067133 -0.238091  0.109774   \n2  0.132654  0.030295  0.102714  0.154334  0.129325 -0.231493  0.007541   \n3  0.150557  0.294355  0.001157  0.285179  0.049340 -0.037548  0.042920   \n4  0.054631 -0.025782  0.049917  0.122052 -0.084216 -0.096424 -0.068681   \n\n    Name_99  \n0  0.300822  \n1 -0.156772  \n2 -0.098540  \n3  0.176173  \n4 -0.005293  \n\n[5 rows x 570 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>Name_90</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.172811</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.245650</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.033781</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.020762</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.191644</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  570 columns</p>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "for i in range(100):\n",
    "    test_df_ohe_mi['Name_' + str(i)] = name_vec[i]\n",
    "\n",
    "test_df_ohe_mi.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:08.121073700Z",
     "start_time": "2023-05-17T06:28:07.182389800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "        0    1    2    3    4    5    6    7    8    9  ...   Name_91  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.098389   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.049657   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.093943   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.149720   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.044182   \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n5761  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.049225   \n5762  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.081374   \n5763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.088004   \n5764  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.041772   \n5765  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.048461   \n\n       Name_92   Name_93   Name_94   Name_95   Name_96   Name_97   Name_98  \\\n0    -0.062941  0.118057 -0.065377  0.227973  0.218879 -0.151266 -0.048105   \n1     0.072740 -0.055925 -0.000046  0.140500  0.067133 -0.238091  0.109774   \n2     0.132654  0.030295  0.102714  0.154334  0.129325 -0.231493  0.007541   \n3     0.150557  0.294355  0.001157  0.285179  0.049340 -0.037548  0.042920   \n4     0.054631 -0.025782  0.049917  0.122052 -0.084216 -0.096424 -0.068681   \n...        ...       ...       ...       ...       ...       ...       ...   \n5761  0.050143  0.039885  0.119543  0.069367  0.129246 -0.127537 -0.031894   \n5762 -0.042311  0.049936  0.215339  0.231080  0.091938 -0.136723  0.014704   \n5763 -0.038802 -0.033024 -0.121403  0.087164  0.142189 -0.052502 -0.033928   \n5764 -0.085859 -0.039293  0.143764  0.066307  0.022076 -0.226420 -0.013429   \n5765 -0.158564 -0.228151  0.039197 -0.176646  0.154168 -0.126946  0.091992   \n\n       Name_99  pagesNumber  \n0     0.300822        118.0  \n1    -0.156772         32.0  \n2    -0.098540        544.0  \n3     0.176173        432.0  \n4    -0.005293        352.0  \n...        ...          ...  \n5761  0.114345        274.0  \n5762  0.113841        224.0  \n5763 -0.002826        224.0  \n5764  0.049143        242.0  \n5765 -0.218716        192.0  \n\n[5766 rows x 571 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>Name_91</th>\n      <th>Name_92</th>\n      <th>Name_93</th>\n      <th>Name_94</th>\n      <th>Name_95</th>\n      <th>Name_96</th>\n      <th>Name_97</th>\n      <th>Name_98</th>\n      <th>Name_99</th>\n      <th>pagesNumber</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.098389</td>\n      <td>-0.062941</td>\n      <td>0.118057</td>\n      <td>-0.065377</td>\n      <td>0.227973</td>\n      <td>0.218879</td>\n      <td>-0.151266</td>\n      <td>-0.048105</td>\n      <td>0.300822</td>\n      <td>118.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.049657</td>\n      <td>0.072740</td>\n      <td>-0.055925</td>\n      <td>-0.000046</td>\n      <td>0.140500</td>\n      <td>0.067133</td>\n      <td>-0.238091</td>\n      <td>0.109774</td>\n      <td>-0.156772</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.093943</td>\n      <td>0.132654</td>\n      <td>0.030295</td>\n      <td>0.102714</td>\n      <td>0.154334</td>\n      <td>0.129325</td>\n      <td>-0.231493</td>\n      <td>0.007541</td>\n      <td>-0.098540</td>\n      <td>544.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.149720</td>\n      <td>0.150557</td>\n      <td>0.294355</td>\n      <td>0.001157</td>\n      <td>0.285179</td>\n      <td>0.049340</td>\n      <td>-0.037548</td>\n      <td>0.042920</td>\n      <td>0.176173</td>\n      <td>432.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.044182</td>\n      <td>0.054631</td>\n      <td>-0.025782</td>\n      <td>0.049917</td>\n      <td>0.122052</td>\n      <td>-0.084216</td>\n      <td>-0.096424</td>\n      <td>-0.068681</td>\n      <td>-0.005293</td>\n      <td>352.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5761</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.049225</td>\n      <td>0.050143</td>\n      <td>0.039885</td>\n      <td>0.119543</td>\n      <td>0.069367</td>\n      <td>0.129246</td>\n      <td>-0.127537</td>\n      <td>-0.031894</td>\n      <td>0.114345</td>\n      <td>274.0</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.081374</td>\n      <td>-0.042311</td>\n      <td>0.049936</td>\n      <td>0.215339</td>\n      <td>0.231080</td>\n      <td>0.091938</td>\n      <td>-0.136723</td>\n      <td>0.014704</td>\n      <td>0.113841</td>\n      <td>224.0</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.088004</td>\n      <td>-0.038802</td>\n      <td>-0.033024</td>\n      <td>-0.121403</td>\n      <td>0.087164</td>\n      <td>0.142189</td>\n      <td>-0.052502</td>\n      <td>-0.033928</td>\n      <td>-0.002826</td>\n      <td>224.0</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.041772</td>\n      <td>-0.085859</td>\n      <td>-0.039293</td>\n      <td>0.143764</td>\n      <td>0.066307</td>\n      <td>0.022076</td>\n      <td>-0.226420</td>\n      <td>-0.013429</td>\n      <td>0.049143</td>\n      <td>242.0</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.048461</td>\n      <td>-0.158564</td>\n      <td>-0.228151</td>\n      <td>0.039197</td>\n      <td>-0.176646</td>\n      <td>0.154168</td>\n      <td>-0.126946</td>\n      <td>0.091992</td>\n      <td>-0.218716</td>\n      <td>192.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  571 columns</p>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat pagesnumber\n",
    "test_df_ohe_mi = pd.concat([test_df_ohe_mi.reset_index(drop=True), test_df['pagesNumber'].reset_index(drop=True)], axis=1)\n",
    "test_df_ohe_mi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:28:09.464036500Z",
     "start_time": "2023-05-17T06:28:09.353894100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\default\\lib\\site-packages\\sklearn\\utils\\validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3., 4.]), array([   4, 5762], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=300, max_depth=25, max_features='auto')\n",
    "random_forest.fit(X_train_ohe_mi, y_train)\n",
    "y_pred = random_forest.predict(test_df_ohe_mi)\n",
    "print(np.unique(y_pred, return_counts=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:52:28.929094700Z",
     "start_time": "2023-05-17T06:51:38.626598200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=['rating_label'])\n",
    "y_pred_df.index += 1\n",
    "y_pred_df.index.name = 'id'\n",
    "y_pred_df.to_csv('book_rating_test_predicted.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:52:35.170445700Z",
     "start_time": "2023-05-17T06:52:35.099386200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "      rating_label\nid                \n1              4.0\n2              4.0\n3              4.0\n4              4.0\n5              4.0\n...            ...\n5762           4.0\n5763           4.0\n5764           4.0\n5765           4.0\n5766           4.0\n\n[5766 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating_label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5762</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>5763</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>5764</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>5765</th>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>5766</th>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5766 rows  1 columns</p>\n</div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:52:35.678191200Z",
     "start_time": "2023-05-17T06:52:35.589929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
